{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a349cd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import emoji\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from matplotlib import pyplot as plt\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import pyLDAvis.gensim_models\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from HanTa import HanoverTagger as ht\n",
    "%matplotlib inline\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('omw-1.4')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5b162d",
   "metadata": {},
   "source": [
    "## Load the new data additionally containing tweets from October 2021 until April 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4c40db65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97899\n",
      "39668\n",
      "31110\n"
     ]
    }
   ],
   "source": [
    "#remove hashtags and only keep tweets with unique texts (keep oldest tweets)\n",
    "df = pd.read_csv ('data/tweets/IchBinHanna_updated.csv')\n",
    "df['new_date'] = pd.to_datetime(df['created_at']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(len(df))\n",
    "#sort by date to ensure duplicate removal keeps oldest tweet\n",
    "df = df.sort_values(by='new_date')\n",
    "df = df.drop_duplicates(subset=['text'], keep='first')\n",
    "print(len(df))\n",
    "df = df.loc[df['reference_type'] != 'retweeted']\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "edf92614",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the data (remove URLs, emojis and line breaks)\n",
    "df['processed'] = df['text'].astype(str)\n",
    "df['processed'] = df['processed'].replace(r'\\\\n',  ' ', regex=True)\n",
    "pat1 = r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+'\n",
    "pat2 = r'www.[^ ]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "split_pattern = re.compile(r'\\b('  + r')\\b')\n",
    "def tweet_cleaner(demo):\n",
    "    soup = BeautifulSoup(demo, 'lxml') # HTML\n",
    "    souped = soup.get_text()\n",
    "    stripped = re.sub(combined_pat, '', souped)\n",
    "    return stripped\n",
    "df['processed'] = [tweet_cleaner(t) for t in df['processed']]\n",
    "def rem_emojis(text):\n",
    "    emojis = [x for x in text if x in emoji.UNICODE_EMOJI]\n",
    "    cleaned = ' '.join([str for str in text.split() if not any(i in str for i in emojis)])\n",
    "    return cleaned\n",
    "df['processed'] = df['processed'].apply(lambda x: rem_emojis(x))\n",
    "df['processed'] = df['processed'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c012fb",
   "metadata": {},
   "source": [
    "## Inspect different stop word lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bc2d11c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens without stopword removal: 64038\n",
      "Unique tokens with initial stopword removal: 63806\n",
      "273\n",
      "1853\n",
      "1879\n",
      "Unique tokens with initial stopword removal + german_stopwords: 62744\n",
      "275\n",
      "414\n",
      "Unique tokens with initial stopword removal + snowball stopwords: 63728\n",
      "Unique tokens with initial stopword removal + both additional lists: 62744\n"
     ]
    }
   ],
   "source": [
    "#preprocessing (tokenization, stop word removal, lemmatizing)\n",
    "german_stop = set(stopwords.words('german'))\n",
    "english_stop = set(stopwords.words('english'))\n",
    "add_stop_all = [\"ichbinhanna\",\"#ichbinhanna\", \"hanna\", \"mehr\", \"innen\", \"#wisszeitvg\", \"#ichbinhannah\", \"@amreibahr\", \"amreibahr\", \"@bmf_bund\",\"bmf_bund\", \"@drkeichhorn\", \"drkeichhorn\", \"@sebastiankubon\", \"sebastiankubon\", \"@bmbf_bund\", \"mehr\", \"innen\", \"schon\", \"gehen\", \"jahr\",\"wissenschaft\", \"wissenschaftler\", \"kommen\",\"academia\", \"academic\", \"year\", \"machen\", \"sagen\", \"sein\",\"geben\", \"also\", \"werden\", \"german\", \"germany\",\"gut\", \"haben\", \"geht\", \"gibt\", \"viele\", \"seit\", \"wäre\", \"sehen\", \"ganz\",\"bekommen\",\"!!!\",\"???\",\"...\"]\n",
    "german_stop.update(set(add_stop_all))\n",
    "english_stop.update(set(add_stop_all))\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "df['tokenized'] = df['processed'].apply(lambda x: tweet_tokenizer.tokenize(x.lower()))\n",
    "dic = Dictionary(df['tokenized'])\n",
    "print('Unique tokens without stopword removal:' ,len(dic))\n",
    "df['tokenized'] = df[['tokenized','lang']].apply(lambda x: ' '.join([word for word in x['tokenized'] if word not in english_stop]).split() if x['lang'] == 'en' else ' '.join([word for word in x['tokenized'] if word not in german_stop]).split(),axis=1)\n",
    "dic_stop = Dictionary(df['tokenized'])\n",
    "print('Unique tokens with initial stopword removal:' ,len(dic_stop))\n",
    "#add the german_stopwords list\n",
    "with open('data/stopwords/german_stopwords.txt', 'r',encoding='utf8') as file:\n",
    "    german_stopwords=[file.read().replace('\\n', ',')]\n",
    "    german_stopwords=german_stopwords[0].split(\",\")\n",
    "print(len(german_stop))\n",
    "add_german_stop = german_stop.copy()\n",
    "add_german_stop.update(set(german_stopwords))\n",
    "print(len(german_stopwords))\n",
    "print(len(add_german_stop))\n",
    "df['tokenized_ger'] = df[['tokenized','lang']].apply(lambda x: ' '.join([word for word in x['tokenized'] if word not in add_german_stop]).split(),axis=1)\n",
    "dic_ger = Dictionary(df['tokenized_ger'])\n",
    "print('Unique tokens with initial stopword removal + german_stopwords:' ,len(dic_ger))\n",
    "#add the snowball stopword list\n",
    "with open('data/stopwords/snowball.txt', 'r',encoding='utf8') as file:\n",
    "    snowball_stopwords=[file.read().replace('\\n', ',')]\n",
    "    snowball_stopwords=snowball_stopwords[0].split(\",\")\n",
    "add_snowball_stop = german_stop.copy()\n",
    "add_snowball_stop.update(set(snowball_stopwords))\n",
    "print(len(snowball_stopwords))\n",
    "print(len(add_snowball_stop))\n",
    "df['tokenized_snow'] = df[['tokenized','lang']].apply(lambda x: ' '.join([word for word in x['tokenized'] if word not in add_snowball_stop]).split(),axis=1)\n",
    "dic_snow = Dictionary(df['tokenized_snow'])\n",
    "print('Unique tokens with initial stopword removal + snowball stopwords:' ,len(dic_snow))\n",
    "#remove both additional stopword lists\n",
    "add_german_stop.update(set(snowball_stopwords))\n",
    "df['tokenized'] = df[['tokenized','lang']].apply(lambda x: ' '.join([word for word in x['tokenized'] if word not in add_german_stop]).split(),axis=1)\n",
    "dic = Dictionary(df['tokenized'])\n",
    "print('Unique tokens with initial stopword removal + both additional lists:' ,len(dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "aa0740e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized'] = df['tokenized'].apply(lambda x: [word for word in x if len(word) > 2])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "hannover = ht.HanoverTagger('morphmodel_ger.pgz')\n",
    "df['lemmatized'] = df[['tokenized','lang']].apply(lambda x: [lemmatizer.lemmatize(word).lower() for word in x['tokenized']] if x['lang'] == 'en' else [hannover.analyze(word)[0].lower() for word in x['tokenized']] ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "842842ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_LDA(tokens, topics=5, passes =5, alpha = 'symmetric', decay = 0.5):\n",
    "    #create the dictionary of lemmatized tokens\n",
    "    dic = Dictionary(tokens)\n",
    "    #print(len(dic))\n",
    "    #remove low and high frequent terms\n",
    "    dic.filter_extremes(no_below=2, no_above=.99)\n",
    "    #print(len(dic))\n",
    "    #create the bag of words \n",
    "    corpus = [dic.doc2bow(d) for d in tokens]\n",
    "    #build LDA model \n",
    "    LDA = LdaMulticore(corpus= corpus, num_topics=topics, id2word= dic, workers=12, passes=passes, alpha = alpha, decay = decay)\n",
    "    words = [re.findall(r'\"([^\"]*)\"',t[1]) for t in LDA.print_topics()]\n",
    "    #create topics\n",
    "    topics = [' '.join(t[0:10]) for t in words]\n",
    "\n",
    "    for id, t in enumerate(topics): \n",
    "        print(f\"------ Topic {id} ------\")\n",
    "        print(t, end=\"\\n\\n\")\n",
    "    # Compute Perplexity\n",
    "    perplexity = LDA.log_perplexity(corpus)\n",
    "    print('\\nPerplexity: ', perplexity) \n",
    "    # Compute Coherence Score\n",
    "    coherence_model = CoherenceModel(model=LDA, texts=tokens, \n",
    "                                   dictionary=dic, coherence='c_v')\n",
    "    coherence_lda_model = coherence_model.get_coherence()\n",
    "    print('\\nCoherence Score: ', coherence_lda_model)\n",
    "    return LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dc0ebee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Topic 0 ------\n",
      "#ichbinreyhan system arbeitsbedingung uni contract phd stellen #wisssystemfehler thread year\n",
      "\n",
      "------ Topic 1 ------\n",
      "#ichbinreyhan arbeit wissen prekär wissenschaftlich karriere thema befristet forschung groß\n",
      "\n",
      "------ Topic 2 ------\n",
      "#ichbinreyhan uni jahr problem @anjakarliczek forschung zeit mensch stellen zeigen\n",
      "\n",
      "------ Topic 3 ------\n",
      "jahr befristet stellen vertrag @gew_bund uni stelle forschung hochschule deutschland\n",
      "\n",
      "------ Topic 4 ------\n",
      "#ichbinreyhan #wisssystemfehler system jahr uni arbeit frage wissen @gew_bund studium\n",
      "\n",
      "\n",
      "Perplexity:  -8.981906687382141\n",
      "\n",
      "Coherence Score:  0.17385792042337486\n"
     ]
    }
   ],
   "source": [
    "full_model = perform_LDA(df['lemmatized'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
