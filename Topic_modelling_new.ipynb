{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a349cd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import emoji\n",
    "import nltk\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import pyLDAvis.gensim_models\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from HanTa import HanoverTagger as ht\n",
    "%matplotlib inline\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('omw-1.4')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5b162d",
   "metadata": {},
   "source": [
    "## Load the new data additionally containing tweets from October 2021 until April 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c40db65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97899\n",
      "39668\n",
      "31110\n"
     ]
    }
   ],
   "source": [
    "#remove hashtags and only keep tweets with unique texts (keep oldest tweets)\n",
    "df = pd.read_csv ('data/tweets/IchBinHanna_updated.csv')\n",
    "df['new_date'] = pd.to_datetime(df['created_at']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(len(df))\n",
    "#sort by date to ensure duplicate removal keeps oldest tweet\n",
    "df = df.sort_values(by='new_date')\n",
    "df = df.drop_duplicates(subset=['text'], keep='first')\n",
    "print(len(df))\n",
    "df = df.loc[df['reference_type'] != 'retweeted']\n",
    "print(len(df))\n",
    "#set random seed\n",
    "seed = 1337"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edf92614",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the data (remove URLs, emojis and line breaks)\n",
    "df['processed'] = df['text'].astype(str)\n",
    "df['processed'] = df['processed'].replace(r'\\\\n',  ' ', regex=True)\n",
    "pat1 = r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+'\n",
    "pat2 = r'www.[^ ]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "split_pattern = re.compile(r'\\b('  + r')\\b')\n",
    "def tweet_cleaner(demo):\n",
    "    soup = BeautifulSoup(demo, 'lxml') # HTML\n",
    "    souped = soup.get_text()\n",
    "    stripped = re.sub(combined_pat, '', souped)\n",
    "    return stripped\n",
    "df['processed'] = [tweet_cleaner(t) for t in df['processed']]\n",
    "def rem_emojis(text):\n",
    "    emojis = [x for x in text if x in emoji.UNICODE_EMOJI]\n",
    "    cleaned = ' '.join([str for str in text.split() if not any(i in str for i in emojis)])\n",
    "    return cleaned\n",
    "df['processed'] = df['processed'].apply(lambda x: rem_emojis(x))\n",
    "df['processed'] = df['processed'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c012fb",
   "metadata": {},
   "source": [
    "## Inspect different stop word lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc2d11c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens without stopword removal: 64038\n",
      "Unique tokens with initial stopword removal: 63806\n",
      "273\n",
      "1853\n",
      "1879\n",
      "Unique tokens with initial stopword removal + german_stopwords: 62744\n",
      "275\n",
      "414\n",
      "Unique tokens with initial stopword removal + snowball stopwords: 63728\n",
      "Unique tokens with initial stopword removal + both additional lists: 62744\n"
     ]
    }
   ],
   "source": [
    "#preprocessing (tokenization, stop word removal, lemmatizing)\n",
    "german_stop = set(stopwords.words('german'))\n",
    "english_stop = set(stopwords.words('english'))\n",
    "add_stop_all = [\"ichbinhanna\",\"#ichbinhanna\",\"#ichbinreyhan\", \"hanna\", \"mehr\", \"innen\", \"#wisszeitvg\", \"#ichbinhannah\", \"@amreibahr\", \"amreibahr\", \"@bmf_bund\",\"bmf_bund\", \"@drkeichhorn\", \"drkeichhorn\", \"@sebastiankubon\", \"sebastiankubon\", \"@bmbf_bund\", \"mehr\", \"innen\", \"schon\", \"gehen\", \"jahr\",\"wissenschaft\", \"wissenschaftler\", \"kommen\",\"academia\", \"academic\", \"year\", \"machen\", \"sagen\", \"sein\",\"geben\", \"also\", \"werden\", \"german\", \"germany\",\"gut\", \"haben\", \"geht\", \"gibt\", \"viele\", \"seit\", \"wäre\", \"sehen\", \"ganz\",\"bekommen\",\"!!!\",\"???\",\"...\"]\n",
    "german_stop.update(set(add_stop_all))\n",
    "english_stop.update(set(add_stop_all))\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "df['tokenized'] = df['processed'].apply(lambda x: tweet_tokenizer.tokenize(x.lower()))\n",
    "dic = Dictionary(df['tokenized'])\n",
    "print('Unique tokens without stopword removal:' ,len(dic))\n",
    "df['tokenized'] = df[['tokenized','lang']].apply(lambda x: ' '.join([word for word in x['tokenized'] if word not in english_stop]).split() if x['lang'] == 'en' else ' '.join([word for word in x['tokenized'] if word not in german_stop]).split(),axis=1)\n",
    "dic_stop = Dictionary(df['tokenized'])\n",
    "print('Unique tokens with initial stopword removal:' ,len(dic_stop))\n",
    "#add the german_stopwords list\n",
    "with open('data/stopwords/german_stopwords.txt', 'r',encoding='utf8') as file:\n",
    "    german_stopwords=[file.read().replace('\\n', ',')]\n",
    "    german_stopwords=german_stopwords[0].split(\",\")\n",
    "print(len(german_stop))\n",
    "add_german_stop = german_stop.copy()\n",
    "add_german_stop.update(set(german_stopwords))\n",
    "print(len(german_stopwords))\n",
    "print(len(add_german_stop))\n",
    "df['tokenized_ger'] = df[['tokenized','lang']].apply(lambda x: ' '.join([word for word in x['tokenized'] if word not in add_german_stop]).split(),axis=1)\n",
    "dic_ger = Dictionary(df['tokenized_ger'])\n",
    "print('Unique tokens with initial stopword removal + german_stopwords:' ,len(dic_ger))\n",
    "#add the snowball stopword list\n",
    "with open('data/stopwords/snowball.txt', 'r',encoding='utf8') as file:\n",
    "    snowball_stopwords=[file.read().replace('\\n', ',')]\n",
    "    snowball_stopwords=snowball_stopwords[0].split(\",\")\n",
    "add_snowball_stop = german_stop.copy()\n",
    "add_snowball_stop.update(set(snowball_stopwords))\n",
    "print(len(snowball_stopwords))\n",
    "print(len(add_snowball_stop))\n",
    "df['tokenized_snow'] = df[['tokenized','lang']].apply(lambda x: ' '.join([word for word in x['tokenized'] if word not in add_snowball_stop]).split(),axis=1)\n",
    "dic_snow = Dictionary(df['tokenized_snow'])\n",
    "print('Unique tokens with initial stopword removal + snowball stopwords:' ,len(dic_snow))\n",
    "#remove both additional stopword lists\n",
    "add_german_stop.update(set(snowball_stopwords))\n",
    "df['tokenized'] = df[['tokenized','lang']].apply(lambda x: ' '.join([word for word in x['tokenized'] if word not in add_german_stop]).split(),axis=1)\n",
    "dic = Dictionary(df['tokenized'])\n",
    "print('Unique tokens with initial stopword removal + both additional lists:' ,len(dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a4a1413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dorther',\n",
       " 'irgendwo',\n",
       " 'äusserstem',\n",
       " 'zB',\n",
       " 'solltet',\n",
       " 'getrennt',\n",
       " 'gratulierte',\n",
       " 'jaehrigem',\n",
       " 'denkbare',\n",
       " 'ehester',\n",
       " 'womit',\n",
       " 'for',\n",
       " 'kürzlichst',\n",
       " 'heutiger',\n",
       " 'letztendlich',\n",
       " 'lichten',\n",
       " 'folgend',\n",
       " 'startet',\n",
       " 'wichtig',\n",
       " 'möglich',\n",
       " 'machte',\n",
       " 'eröffnetes',\n",
       " 'mittig',\n",
       " 'angesetzten',\n",
       " 'dagegen',\n",
       " 'allgemeinste',\n",
       " 'irgendwas',\n",
       " 'direkt',\n",
       " 'mithin',\n",
       " 'sei',\n",
       " 'überll',\n",
       " 'worin',\n",
       " 'unerhoerter',\n",
       " 'etlichem',\n",
       " 'beträchtliches',\n",
       " 'author',\n",
       " 'sieben',\n",
       " 'daher',\n",
       " 'neun',\n",
       " 'dahingehendes',\n",
       " 'ploetzlichem',\n",
       " 'unerhörte',\n",
       " 'jährige',\n",
       " 'ähnlichen',\n",
       " 'häufigem',\n",
       " 'persoenlich',\n",
       " 'weiterem',\n",
       " 'geworden',\n",
       " 'letztes',\n",
       " 'nachher',\n",
       " 'allgemeiner',\n",
       " 'damals',\n",
       " 'ebenfalls',\n",
       " 'deshalb',\n",
       " 'augenscheinlichsten',\n",
       " 'jährigem',\n",
       " 'somit',\n",
       " 'versorgte',\n",
       " 'weitere',\n",
       " 'damaliges',\n",
       " 'ueber',\n",
       " 'naturgemäss',\n",
       " 'richtiggehender',\n",
       " 'mehrfach',\n",
       " 'tut',\n",
       " 'seien',\n",
       " 'allgemeinem',\n",
       " 'muessen',\n",
       " 'gängiger',\n",
       " 'immerwaehrendem',\n",
       " 'vorherig',\n",
       " 'hierbei',\n",
       " 'bezgl.',\n",
       " 'irgend',\n",
       " 'eigentlich',\n",
       " 'anstatt',\n",
       " 'nötigenfalls',\n",
       " 'infolge',\n",
       " 'woher',\n",
       " 'jenseitigem',\n",
       " 'freie',\n",
       " 'unsen',\n",
       " 'wodurch',\n",
       " 'meistenteils',\n",
       " 'mag',\n",
       " 'bestimmt',\n",
       " 'vollstaendige',\n",
       " 'koennten',\n",
       " 'zog',\n",
       " 'bestimmtem',\n",
       " 'meisten',\n",
       " 'sowohl',\n",
       " 'vielen',\n",
       " 'womöglich',\n",
       " 'augenscheinlich',\n",
       " 'zuerst',\n",
       " 'mittelst',\n",
       " 'richtiggehenden',\n",
       " 'gleichsam',\n",
       " 'gleiche',\n",
       " 'gehabt',\n",
       " 'dunklen',\n",
       " 'ihrige',\n",
       " 'ähnlichstes',\n",
       " 'geehrten',\n",
       " 'geehrt',\n",
       " 'fortsetzte',\n",
       " 'andauerndem',\n",
       " 'allgemeines',\n",
       " 'verraten',\n",
       " 'bestimmter',\n",
       " 'obs',\n",
       " 'links',\n",
       " 'haeufigeres',\n",
       " 'berichteten',\n",
       " 'bekannt',\n",
       " 'durchwegs',\n",
       " 'meinetwegen',\n",
       " 'umständehalber',\n",
       " 'schlechter',\n",
       " 'hinterm',\n",
       " 'unerhoertes',\n",
       " 'teilten',\n",
       " 'zweifelsfreiem',\n",
       " 'unterbrach',\n",
       " 'unmoegliches',\n",
       " 'ueberall',\n",
       " 'tät',\n",
       " 'keinerlei',\n",
       " 'mache',\n",
       " 'irgendwen',\n",
       " 'eigentliches',\n",
       " 'würdest',\n",
       " 'reichlich',\n",
       " 'bestimmte',\n",
       " 'stellenweise',\n",
       " 'beträchtlicher',\n",
       " 'ebenso',\n",
       " 'abermaliges',\n",
       " 'allenthalben',\n",
       " 'ungewöhnliche',\n",
       " 'ncht',\n",
       " 'junge',\n",
       " 'ziemliches',\n",
       " 'beträchtliche',\n",
       " 'dritte',\n",
       " 'fortsetzt',\n",
       " 'reichlichen',\n",
       " 'gleichzeitiges',\n",
       " 'unsägliche',\n",
       " 'oefter',\n",
       " 'häufige',\n",
       " 'bekennen',\n",
       " 'ausgerechnetem',\n",
       " 'beisammen',\n",
       " 'offensichtlichem',\n",
       " 'langsam',\n",
       " 'letztlich',\n",
       " 'ähnlichster',\n",
       " 'ergänzten',\n",
       " 'befragten',\n",
       " 'offenkundiger',\n",
       " 'beide',\n",
       " 'geehrter',\n",
       " 'finde',\n",
       " 'gib',\n",
       " 'weit',\n",
       " 'abgerufener',\n",
       " 'vielmals',\n",
       " 'wart',\n",
       " 'allerlei',\n",
       " 'restlosen',\n",
       " 'schlussendlich',\n",
       " 'allein',\n",
       " 'sobald',\n",
       " 'reagieren',\n",
       " 'junger',\n",
       " 'hinunter',\n",
       " 'tatsächlichen',\n",
       " 'wessen',\n",
       " 'allg',\n",
       " 'entsprechendes',\n",
       " 'nächste',\n",
       " 'konnten',\n",
       " 'statt',\n",
       " 'hindurch',\n",
       " 'zieht',\n",
       " 'wirklichem',\n",
       " 'starteten',\n",
       " 'ansonst',\n",
       " 'Dat',\n",
       " 'unmöglich',\n",
       " 'gebracht',\n",
       " 'schnell',\n",
       " 'zb.',\n",
       " 'empfunden',\n",
       " 'weitgehender',\n",
       " 'verrate',\n",
       " 'natürlich',\n",
       " 'aufgrund',\n",
       " 'wohin',\n",
       " 'gleichen',\n",
       " 'fünf',\n",
       " 'schwerliches',\n",
       " 'unerhört',\n",
       " 'folgendem',\n",
       " 'entlang',\n",
       " 'wobei',\n",
       " 'ersterem',\n",
       " 'ueberallhin',\n",
       " 'pro',\n",
       " 'ungewoehnlichem',\n",
       " 'anerkannter',\n",
       " 'zweifelsfrei',\n",
       " 'solch',\n",
       " 'begann',\n",
       " 'bleiben',\n",
       " 'tatsaechlicher',\n",
       " 'gleichem',\n",
       " 'unmassgeblichen',\n",
       " 'neuer',\n",
       " 'direkten',\n",
       " 'solc hen',\n",
       " 'insgeheimer',\n",
       " 'möglichst',\n",
       " 'gekonnt',\n",
       " 'mehrmaligem',\n",
       " 'mittels',\n",
       " 'demgegenueber',\n",
       " 'dafür',\n",
       " 'woraus',\n",
       " 'wann',\n",
       " 'unmoeglich',\n",
       " 'blieb',\n",
       " 'einbaün',\n",
       " 'etliches',\n",
       " 'plötzliches',\n",
       " 'versorgtes',\n",
       " 'senke',\n",
       " 'naturgemaess',\n",
       " 'massgebendes',\n",
       " 'ungewoehnliche',\n",
       " 'eheste',\n",
       " 'allenfalls',\n",
       " 'halb',\n",
       " 'unsres',\n",
       " 'andauernde',\n",
       " 'brachten',\n",
       " 'gaenzlichem',\n",
       " 'weitestgehende',\n",
       " 'tat',\n",
       " 'z.B.',\n",
       " 'dannen',\n",
       " 'fordert',\n",
       " 'mehrere',\n",
       " 'beinahe',\n",
       " 'ungleich',\n",
       " 'dahingehender',\n",
       " 'unsaegliches',\n",
       " 'hinterher',\n",
       " 'tatsaechliches',\n",
       " 'niemandes',\n",
       " 'ehe',\n",
       " 'durftest',\n",
       " 'setzt',\n",
       " 'betraechtliche',\n",
       " 'siehe',\n",
       " 'einigermaßen',\n",
       " 'haeufigeren',\n",
       " 'ungewoehnlicher',\n",
       " 'ausgenommenem',\n",
       " 'vermag',\n",
       " 'niemandem',\n",
       " 'leer',\n",
       " 'bearbeiten',\n",
       " 'erneut',\n",
       " 'ganzer',\n",
       " 'hinein',\n",
       " 'allzu',\n",
       " 'sage',\n",
       " 'wohlweislich',\n",
       " 'hinlanglich',\n",
       " 'information',\n",
       " 'jaehriger',\n",
       " 'immerwährende',\n",
       " 'immer',\n",
       " 'danach',\n",
       " 'seltsamerweise',\n",
       " 'aeusserstem',\n",
       " 'fand',\n",
       " 'offensichtlicher',\n",
       " 'diesseitiges',\n",
       " 'schreiben',\n",
       " 'aeusserstes',\n",
       " 'eigentliche',\n",
       " 'irgendwann',\n",
       " 'gefiel',\n",
       " 'unsem',\n",
       " 'weiteren',\n",
       " 'gesagt',\n",
       " 'finden',\n",
       " 'gern',\n",
       " 'derartigen',\n",
       " 'ohnedies',\n",
       " 'derartigem',\n",
       " 'weitestgehender',\n",
       " 'deswegen',\n",
       " 'jaehrigen',\n",
       " 'jähriges',\n",
       " 'selbstredender',\n",
       " 'genug',\n",
       " 'massgebliche',\n",
       " 'oberen',\n",
       " 'darum',\n",
       " 'ausdrücklichem',\n",
       " 'waehrend',\n",
       " 'anscheinend',\n",
       " 'unbedingter',\n",
       " 'erhielt',\n",
       " 'fort',\n",
       " 'folgender',\n",
       " 'autor',\n",
       " 'frau',\n",
       " 'denkbares',\n",
       " 'suchen',\n",
       " 'haeufige',\n",
       " 'ploetzliche',\n",
       " 'sollen',\n",
       " 'schätzte',\n",
       " 'sowieso',\n",
       " 'überallhin',\n",
       " 'ziemlichen',\n",
       " 'bearbeitete',\n",
       " 'voelliger',\n",
       " 'betreffende',\n",
       " 'weiterer',\n",
       " 'bedurfte',\n",
       " 'berichtet',\n",
       " 'darüber',\n",
       " 'oftmals',\n",
       " 'betreffender',\n",
       " 'junges',\n",
       " 'unerhörten',\n",
       " 'fall',\n",
       " 'zeitweisen',\n",
       " 'erscheinen',\n",
       " 'circa',\n",
       " 'weitestgehenden',\n",
       " 'davon',\n",
       " 'allg.',\n",
       " 'womögliches',\n",
       " 'vorher',\n",
       " 'immerwährend',\n",
       " 'daraus',\n",
       " 'dadurch',\n",
       " 'insbesondere',\n",
       " 'weitgehende',\n",
       " 'unsrem',\n",
       " 'unstreitigem',\n",
       " 'damaligen',\n",
       " 'gepriesenes',\n",
       " 'ungewöhnlich',\n",
       " 'anderst',\n",
       " 'ueberhaupt',\n",
       " 'nacher',\n",
       " 'beiden',\n",
       " 'sonstwo',\n",
       " 'berichten',\n",
       " 'immerwaehrende',\n",
       " 'ja',\n",
       " 'behielt',\n",
       " 'gänzlich',\n",
       " 'neu',\n",
       " 'sagtest',\n",
       " 'derzeitig',\n",
       " 'versorgen',\n",
       " 'falls',\n",
       " 'befragte',\n",
       " 'schreibens',\n",
       " 'scheinbar',\n",
       " 'gleichste',\n",
       " 'musst',\n",
       " 'irgendwelche',\n",
       " 'ausnahmslose',\n",
       " 'demgemäss',\n",
       " 'bsp.',\n",
       " 'schwerliche',\n",
       " 'lagen',\n",
       " 'womöglichem',\n",
       " 'brachte',\n",
       " 'wurdest',\n",
       " 'zweifelsfreies',\n",
       " 'and',\n",
       " 'unzweifelhaftem',\n",
       " 'vollstaendiger',\n",
       " 'möchten',\n",
       " 'hiermit',\n",
       " 'unmöglicher',\n",
       " 'möglicherweise',\n",
       " 'heutigen',\n",
       " 'unterbrechen',\n",
       " 'gemaess',\n",
       " 'außen',\n",
       " 'durchaus',\n",
       " 'txt',\n",
       " 'ungleichem',\n",
       " 'bspw',\n",
       " 'bestimmtes',\n",
       " 'derzeitigen',\n",
       " 'gänzliche',\n",
       " 'gegenüber',\n",
       " 'leider',\n",
       " 'vollständig',\n",
       " 'konkretes',\n",
       " 'einzig',\n",
       " 'vermutlichem',\n",
       " 'vier',\n",
       " 'eigentlicher',\n",
       " 'jeglichem',\n",
       " 'übrig',\n",
       " 'findet',\n",
       " 'anderweitige',\n",
       " 'völligen',\n",
       " 'möglicher',\n",
       " 'her',\n",
       " 'ähnlicher',\n",
       " 'mancherorts',\n",
       " 'version',\n",
       " 'allgemeinen',\n",
       " 'bekanntlicher',\n",
       " 'mehrmaliges',\n",
       " 'bekannter',\n",
       " 'bekanntlichen',\n",
       " 'offenkundigen',\n",
       " 'vorherigem',\n",
       " 'wollten',\n",
       " 'wirkliches',\n",
       " 'igitt',\n",
       " 'koennte',\n",
       " 'wuerdet',\n",
       " 'oberhalb',\n",
       " 'ausgerechnet',\n",
       " 'ähnlich',\n",
       " 'hundert',\n",
       " 'müssten',\n",
       " 'x',\n",
       " 'moeglichstem',\n",
       " 'persönlich',\n",
       " 'kuerzlich',\n",
       " 'selbe',\n",
       " 'übel',\n",
       " 'folgende',\n",
       " 'bekannte',\n",
       " 'unterhalb',\n",
       " 'höchst',\n",
       " 'bezueglich',\n",
       " 'vollständigem',\n",
       " 'äusserster',\n",
       " 'moeglichsten',\n",
       " 'wenigstens',\n",
       " 'plötzlicher',\n",
       " 'beispielsweise',\n",
       " 'nämlich',\n",
       " 'hätten',\n",
       " 'derzeit',\n",
       " 'komme',\n",
       " 'gemäss',\n",
       " 'ploetzlich',\n",
       " 'unsaeglichen',\n",
       " 'freier',\n",
       " 'eigenen',\n",
       " 'unerhoerten',\n",
       " 'restloses',\n",
       " 'wohl',\n",
       " 'betraechtlicher',\n",
       " 'konntet',\n",
       " 'gewissem',\n",
       " 'folgendermassen',\n",
       " 'findest',\n",
       " 'bzw.',\n",
       " 'gemacht',\n",
       " 'vermutliche',\n",
       " 'durftet',\n",
       " 'vermutliches',\n",
       " 'währenddessen',\n",
       " 'womoegliche',\n",
       " 'darfst',\n",
       " 'liest',\n",
       " 'gaenzlich',\n",
       " 'beides',\n",
       " 'gbr',\n",
       " 'sog',\n",
       " 'haeufigerer',\n",
       " 'genau',\n",
       " 'grösstenteils',\n",
       " 'sollt',\n",
       " 'nimmer',\n",
       " 'zugleich',\n",
       " 'zurueck',\n",
       " 'www',\n",
       " 'jemals',\n",
       " 'einmaligem',\n",
       " 'wer',\n",
       " 'niemals',\n",
       " 'selbstredend',\n",
       " 'unmöglichen',\n",
       " 'desto',\n",
       " 'gegenueber',\n",
       " 'wiederum',\n",
       " 'dürften',\n",
       " 'sollst',\n",
       " 'sicherlich',\n",
       " 'anderweitigen',\n",
       " 'wohingegen',\n",
       " 'aehnliche',\n",
       " 'irgendeiner',\n",
       " 'voellige',\n",
       " 'daneben',\n",
       " 'morgige',\n",
       " 'einmaliges',\n",
       " 'sect',\n",
       " 'uebrigens',\n",
       " 'unsäglicher',\n",
       " 'allerdings',\n",
       " 'klare',\n",
       " 'einmaligen',\n",
       " 'klein',\n",
       " 'ergänzen',\n",
       " 'ausdrücklicher',\n",
       " 'bloss',\n",
       " 'eigenes',\n",
       " 'dran',\n",
       " 'abgerufene',\n",
       " 'wolle',\n",
       " 'sieht',\n",
       " 'drum',\n",
       " 'massgeblich',\n",
       " 'ausdrückliche',\n",
       " 'acht',\n",
       " 'nein',\n",
       " 'vielerlei',\n",
       " 'herein',\n",
       " 'jemandem',\n",
       " 'gleichzeitiger',\n",
       " 'seither',\n",
       " 'bräuchte',\n",
       " 'irgendwohin',\n",
       " 'bsp',\n",
       " 'längstens',\n",
       " 'dahingehende',\n",
       " 'ums',\n",
       " 'sechs',\n",
       " 'insgesamte',\n",
       " 'veröffentlicht',\n",
       " 'lesen',\n",
       " 'etlicher',\n",
       " 'gaenzliches',\n",
       " 'kam',\n",
       " 'unsagbar',\n",
       " 'fragte',\n",
       " 'aehnlichstem',\n",
       " 'geb',\n",
       " 'fortsetzen',\n",
       " 'dinge',\n",
       " 'hiesiges',\n",
       " 'hallo',\n",
       " 'irgendwie',\n",
       " 'ausgerechneter',\n",
       " 'ausgenommen',\n",
       " 'eigener',\n",
       " 'wieviele',\n",
       " 'befiehlte',\n",
       " 'unteres',\n",
       " 'schreibe',\n",
       " 'tust',\n",
       " 'konntest',\n",
       " 'ausnahmslosem',\n",
       " 'häufigeren',\n",
       " 'ungewoehnlichen',\n",
       " 'ungewöhnliches',\n",
       " 'voelligem',\n",
       " 'zumeist',\n",
       " 'wen',\n",
       " 'nichtsdestotrotz',\n",
       " 'gewissermaßen',\n",
       " 'spielen',\n",
       " 'aehnlichster',\n",
       " 'massgebend',\n",
       " 'sagte',\n",
       " 'wuerden',\n",
       " 'veröffentlicher',\n",
       " 'irgendeine',\n",
       " 'offensichtliche',\n",
       " 'herum',\n",
       " 'womoeglichem',\n",
       " 'entsprechende',\n",
       " 'unsagbares',\n",
       " 'sagtet',\n",
       " 'wenig',\n",
       " 'vollstaendig',\n",
       " 'letzte',\n",
       " 'bisschen',\n",
       " 'tatsächliches',\n",
       " 'getragen',\n",
       " 'irgendwenn',\n",
       " 'zumindest',\n",
       " 'ueberdies',\n",
       " 'startete',\n",
       " 'naechste',\n",
       " 'jenseits',\n",
       " 'wachen',\n",
       " 'neuerdings',\n",
       " 'stiegen',\n",
       " 'entgegen',\n",
       " 'ganzem',\n",
       " 'sattsam',\n",
       " 'been',\n",
       " 'irgendeines',\n",
       " 'bisheriger',\n",
       " 'möglichstem',\n",
       " 'allgemeinster',\n",
       " 'beträchtlichem',\n",
       " 'nehmen',\n",
       " 'ziemlichem',\n",
       " 'setzte',\n",
       " 'unstreitiges',\n",
       " 'anerkanntes',\n",
       " 'reichlicher',\n",
       " 'immerwährenden',\n",
       " 'dreißig',\n",
       " 'doppelt',\n",
       " 'freies',\n",
       " 'schätzten',\n",
       " 'kaum',\n",
       " 'quasi',\n",
       " 'gleichzeitig',\n",
       " 'nie',\n",
       " 'steht',\n",
       " 'gute',\n",
       " 'gruendlich',\n",
       " 'mindestens',\n",
       " 'ungewöhnlicher',\n",
       " 'entsprechender',\n",
       " 'unbedingt',\n",
       " 'stieg',\n",
       " 'bislang',\n",
       " 'teilen',\n",
       " 'aehnlichen',\n",
       " 'letze',\n",
       " 'böden',\n",
       " 'selbstredendem',\n",
       " 'unbedingtes',\n",
       " 'gänzliches',\n",
       " 'wohlgemerkt',\n",
       " 'senken',\n",
       " 'verrieten',\n",
       " 'diejenige',\n",
       " 'sicher',\n",
       " 'anfangen',\n",
       " 'ehesten',\n",
       " 'reichlichem',\n",
       " 'abermals',\n",
       " 'teile',\n",
       " 'getan',\n",
       " 'weitgehendem',\n",
       " 'zumal',\n",
       " 'trägt',\n",
       " 'bekanntlichem',\n",
       " 'befragen',\n",
       " 'teilte',\n",
       " 'später',\n",
       " 'ausserdem',\n",
       " 'wolltest',\n",
       " 'veröffentlichen',\n",
       " 'einfach',\n",
       " 'hintendran',\n",
       " 'augenscheinlichste',\n",
       " 'gab',\n",
       " 'ihriges',\n",
       " 'unmoeglicher',\n",
       " 'immerwährender',\n",
       " 'darf',\n",
       " 'aehnlichem',\n",
       " 'ersteres',\n",
       " 'wären',\n",
       " 'gefälligst',\n",
       " 'gemeinhin',\n",
       " 'forderten',\n",
       " 'derjenige',\n",
       " 'gekommen',\n",
       " 'ähnlichste',\n",
       " 'derjenigen',\n",
       " 'abgerufenes',\n",
       " 'derzeitigem',\n",
       " 'steigen',\n",
       " 'manch',\n",
       " 'gluecklicherweise',\n",
       " 'weitestgehendem',\n",
       " 'gleiches',\n",
       " 'befohlens',\n",
       " 'erstes',\n",
       " 'ans',\n",
       " 'gründlich',\n",
       " 'ober',\n",
       " 'ab',\n",
       " 'aehnlich',\n",
       " 'sollten',\n",
       " 'geblieben',\n",
       " 'rundheraus',\n",
       " 'insgesamter',\n",
       " 'ausnahmslos',\n",
       " 'aeusserster',\n",
       " 'richtiggehendem',\n",
       " 'duerfte',\n",
       " 'bevor',\n",
       " 'etwaige',\n",
       " 'unerhörtem',\n",
       " 'etlichen',\n",
       " 'völliger',\n",
       " 'liegt',\n",
       " 'allgemeinsten',\n",
       " 'vorheriger',\n",
       " 'häufig',\n",
       " 'ersterer',\n",
       " 'vielleicht',\n",
       " 'bedarf',\n",
       " 'weswegen',\n",
       " 'habt',\n",
       " 'legte',\n",
       " 'hinlänglich',\n",
       " 'unse',\n",
       " 'einseitige',\n",
       " 'sagten',\n",
       " 'höchstens',\n",
       " 'unsäglich',\n",
       " 'aufhören',\n",
       " 'womöglicher',\n",
       " 'entweder',\n",
       " 'zahlreich',\n",
       " 'letzten',\n",
       " 'schlicht',\n",
       " 'magst',\n",
       " 'zuviele',\n",
       " 'übrigens',\n",
       " 'zufolge',\n",
       " 'sogleich',\n",
       " 'unsrer',\n",
       " 'gefallen',\n",
       " 'soweit',\n",
       " 'aeussersten',\n",
       " 'betraechtliches',\n",
       " 'denkbarem',\n",
       " 'miteinander',\n",
       " 'senkte',\n",
       " 'neuen',\n",
       " 'konkreter',\n",
       " 'unsägliches',\n",
       " 'zirka',\n",
       " 'besserem',\n",
       " 'dahin',\n",
       " 'moeglichste',\n",
       " 'sodaß',\n",
       " 'völlige',\n",
       " 'ähnliches',\n",
       " 'erstem',\n",
       " 'neuem',\n",
       " 'genommen',\n",
       " 'massgeblicher',\n",
       " 'darüberhinaus',\n",
       " 'nutzt',\n",
       " 'rundum',\n",
       " 'kuerzlichst',\n",
       " 'unzweifelhafte',\n",
       " 'einmalig',\n",
       " 'uebrig',\n",
       " 'keineswegs',\n",
       " 'könnten',\n",
       " 'folgendes',\n",
       " 'important',\n",
       " 'http',\n",
       " 'unmögliche',\n",
       " 'unerhoert',\n",
       " 'irgendeinem',\n",
       " 'derzeitiges',\n",
       " 'nachhinein',\n",
       " 'direkte',\n",
       " 'euretwegen',\n",
       " 'innerlich',\n",
       " 'anderenfalls',\n",
       " 'leicht',\n",
       " 'müsste',\n",
       " 'anderweitig',\n",
       " 'bezgl',\n",
       " 'geteilte',\n",
       " 'umso',\n",
       " 'bisherigem',\n",
       " 'zweifelsfreie',\n",
       " 'ungleichen',\n",
       " 'voelliges',\n",
       " 'womöglichen',\n",
       " 'senkt',\n",
       " 'deren',\n",
       " 'ueblicherweise',\n",
       " 'erste',\n",
       " 'jaehrige',\n",
       " 'seitdem',\n",
       " 'zusammen',\n",
       " 'hattest',\n",
       " 'betraechtlichem',\n",
       " 'vergangen',\n",
       " 'obwohl',\n",
       " 'allgemeine',\n",
       " 'bleibt',\n",
       " 'abgerufen',\n",
       " 'beträchtlich',\n",
       " 'braucht',\n",
       " 'unmoeglichem',\n",
       " 'einseitig',\n",
       " 'wirklicher',\n",
       " 'mitten',\n",
       " 'such',\n",
       " 'weshalb',\n",
       " 'info',\n",
       " 'eigenem',\n",
       " 'darueberhinaus',\n",
       " 'kleines',\n",
       " 'vollständige',\n",
       " 'ganzen',\n",
       " 'oefters',\n",
       " 'besonders',\n",
       " 'denkbar',\n",
       " 'etc',\n",
       " 'reagierte',\n",
       " 'möglichsten',\n",
       " 'oft',\n",
       " 'meins',\n",
       " 'solltest',\n",
       " 'titel',\n",
       " 'darueber',\n",
       " 'samt',\n",
       " 'persoenliches',\n",
       " 'neuerlicher',\n",
       " 'gleichstem',\n",
       " 'persönliches',\n",
       " 'mitwohl',\n",
       " 'gleichzeitigem',\n",
       " 'geteilt',\n",
       " 'klar',\n",
       " 'hätt',\n",
       " 'erhielten',\n",
       " 'oberste',\n",
       " 'drunter',\n",
       " 'konkret',\n",
       " 'gaenzliche',\n",
       " 'völliges',\n",
       " 'bekanntliche',\n",
       " 'haeufiger',\n",
       " 'bald',\n",
       " 'wieso',\n",
       " 'jungen',\n",
       " 'ggf',\n",
       " 'entsprechend',\n",
       " 'gängige',\n",
       " 'ausgenommenes',\n",
       " 'obschon',\n",
       " 'unsaegliche',\n",
       " 'gewisses',\n",
       " 'weitestgehendes',\n",
       " 'betraechtlich',\n",
       " 'möchte',\n",
       " 'hattet',\n",
       " 'künftig',\n",
       " 'mitnichten',\n",
       " 'käumlich',\n",
       " 'gratulieren',\n",
       " 'ausdrückt',\n",
       " 'ca',\n",
       " 'aufhörte',\n",
       " 'völligst',\n",
       " 'tragen',\n",
       " 'mehrerer',\n",
       " 'jedesmal',\n",
       " 'augenscheinliches',\n",
       " 'zeitweiser',\n",
       " 'selben',\n",
       " 'haeufigem',\n",
       " 'arbeiten',\n",
       " 'kuenftig',\n",
       " 'unmoeglichen',\n",
       " 'behalten',\n",
       " 'wurdet',\n",
       " 'ende',\n",
       " 'recht',\n",
       " 'sämtliche',\n",
       " 'begonnen',\n",
       " 'gleichster',\n",
       " 'mochte',\n",
       " 'sonstige',\n",
       " 'insgesamt',\n",
       " 'rund',\n",
       " 'unstreitiger',\n",
       " 'gewollt',\n",
       " 'immerwaehrender',\n",
       " 'diejenigen',\n",
       " 'toll',\n",
       " 'seiest',\n",
       " 'einseitiger',\n",
       " 'jaehriges',\n",
       " 'persönliche',\n",
       " 'unterster',\n",
       " 'hiesigen',\n",
       " 'einstmals',\n",
       " 'könntet',\n",
       " 'unsaeglichem',\n",
       " 'unzweifelhaft',\n",
       " 'unsagbarer',\n",
       " 'anfing',\n",
       " 'gefällt',\n",
       " 'sonstiger',\n",
       " 'schätzt',\n",
       " 'unterste',\n",
       " 'wofür',\n",
       " 'andauerndes',\n",
       " 'wär',\n",
       " 'ungewöhnlichem',\n",
       " 'bezüglich',\n",
       " 'nimm',\n",
       " 'aehnliches',\n",
       " 'mögliche',\n",
       " 'sofern',\n",
       " 'durfte',\n",
       " 'ungewoehnliches',\n",
       " 'veröffentlichtes',\n",
       " 'sangen',\n",
       " 'neben',\n",
       " 'vergangenes',\n",
       " 'duerftet',\n",
       " 'massgebendem',\n",
       " 'ungleiches',\n",
       " 'indessen',\n",
       " 'gegeben',\n",
       " 'unerhörtes',\n",
       " 'derart',\n",
       " 'vollständiger',\n",
       " 'jemandes',\n",
       " 'setzen',\n",
       " 'derartiger',\n",
       " 'immerwährendem',\n",
       " 'jederlei',\n",
       " 'niemanden',\n",
       " 'trage',\n",
       " 'erhalten',\n",
       " 'legten',\n",
       " 'unmassgeblich',\n",
       " 'worden',\n",
       " 'einführen',\n",
       " 'hoch',\n",
       " 'drei',\n",
       " 'augenscheinlichst',\n",
       " 'drauf',\n",
       " 'offenkundige',\n",
       " 'gänzlichem',\n",
       " 'heute',\n",
       " 'ausgenommene',\n",
       " 'womoeglicher',\n",
       " 'ausdrücklichen',\n",
       " 'warum',\n",
       " 'immerwährendes',\n",
       " 'ausnahmsloser',\n",
       " 'sofort',\n",
       " 'gilt',\n",
       " 'woran',\n",
       " 'daran',\n",
       " 'unstreitige',\n",
       " 'machten',\n",
       " 'danken',\n",
       " 'usw',\n",
       " 'allzeit',\n",
       " 'schlichtweg',\n",
       " 'zahlreichem',\n",
       " 'zogen',\n",
       " 'zweifelsfreier',\n",
       " 'solange',\n",
       " 'voran',\n",
       " 'hintern',\n",
       " 'soeben',\n",
       " 'augenscheinlichem',\n",
       " 'diesseitige',\n",
       " 'überaus',\n",
       " 'denkbarer',\n",
       " 'stattdessen',\n",
       " 'massgeblichem',\n",
       " 'nutzung',\n",
       " 'versorgt',\n",
       " 'äussersten',\n",
       " 'ggf.',\n",
       " 'persoenlichem',\n",
       " 'persoenlicher',\n",
       " 'ähnliche',\n",
       " 'unsäglichen',\n",
       " ...}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(german_stopwords)-german_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa0740e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized'] = df['tokenized'].apply(lambda x: [word for word in x if len(word) > 2])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "hannover = ht.HanoverTagger('morphmodel_ger.pgz')\n",
    "df['lemmatized'] = df[['tokenized','lang']].apply(lambda x: [lemmatizer.lemmatize(word).lower() for word in x['tokenized']] if x['lang'] == 'en' else [hannover.analyze(word)[0].lower() for word in x['tokenized']] ,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd12e2a0",
   "metadata": {},
   "source": [
    "## Perform previously used LDA approach on full, german, english and per month data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "842842ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_LDA(tokens, topics=5, passes =5, alpha = 'symmetric', decay = 0.5):\n",
    "    #create the dictionary of lemmatized tokens\n",
    "    dic = Dictionary(tokens)\n",
    "    #print(len(dic))\n",
    "    #remove low and high frequent terms\n",
    "    dic.filter_extremes(no_below=2, no_above=.99)\n",
    "    #print(len(dic))\n",
    "    #create the bag of words \n",
    "    corpus = [dic.doc2bow(d) for d in tokens]\n",
    "    #build LDA model \n",
    "    LDA = LdaMulticore(corpus= corpus, num_topics=topics, id2word= dic, workers=12, passes=passes, alpha = alpha, decay = decay)\n",
    "    words = [re.findall(r'\"([^\"]*)\"',t[1]) for t in LDA.print_topics()]\n",
    "    #create topics\n",
    "    topics = [' '.join(t[0:10]) for t in words]\n",
    "\n",
    "    for id, t in enumerate(topics): \n",
    "        print(f\"------ Topic {id} ------\")\n",
    "        print(t, end=\"\\n\\n\")\n",
    "    # Compute Perplexity\n",
    "    perplexity = LDA.log_perplexity(corpus)\n",
    "    print('\\nPerplexity: ', perplexity) \n",
    "    # Compute Coherence Score\n",
    "    coherence_model = CoherenceModel(model=LDA, texts=tokens, \n",
    "                                   dictionary=dic, coherence='c_v')\n",
    "    coherence_lda_model = coherence_model.get_coherence()\n",
    "    print('\\nCoherence Score: ', coherence_lda_model)\n",
    "    return LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc0ebee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Topic 0 ------\n",
      "#ichbinreyhan system thread research @gew_bund problem year frage arbeit contract\n",
      "\n",
      "------ Topic 1 ------\n",
      "#ichbinreyhan uni hochschule lehre forschung #frististfrust jahr beschäftigt zeit richtig\n",
      "\n",
      "------ Topic 2 ------\n",
      "stellen wissenschaftlich befristet zeit promotion stelle arbeit woche uni problem\n",
      "\n",
      "------ Topic 3 ------\n",
      "stellen befristet jahr wissen kind problem @anjakarliczek #hannaimbundestag #ichbinreyhan arbeitsbedingung\n",
      "\n",
      "------ Topic 4 ------\n",
      "#ichbinreyhan jahr #wisssystemfehler uni forschung system arbeit problem thema @gew_bund\n",
      "\n",
      "\n",
      "Perplexity:  -9.016812370242793\n",
      "\n",
      "Coherence Score:  0.2001953135888185\n"
     ]
    }
   ],
   "source": [
    "full_model = perform_LDA(df['lemmatized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "453ab15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Topic 0 ------\n",
      "uni problem zeit arbeitsbedingung prekär deutsch wissenschaftlich thema #ichbinreyhan vertrag\n",
      "\n",
      "------ Topic 1 ------\n",
      "jahr forschung buch problem zeit arbeitsbedingung wissenschaftlich lehre #ichbinreyhan promotion\n",
      "\n",
      "------ Topic 2 ------\n",
      "#ichbinreyhan prekär @gew_bund #dauerstell deutschland befristet @suhrkamp jahr stellen hochschule\n",
      "\n",
      "------ Topic 3 ------\n",
      "#ichbinreyhan jahr uni forschung arbeit stellen system stelle gut wichtig\n",
      "\n",
      "------ Topic 4 ------\n",
      "#ichbinreyhan arbeit problem thema uni universität schaffen mensch hochschule prekär\n",
      "\n",
      "\n",
      "Perplexity:  -8.983359045768525\n",
      "\n",
      "Coherence Score:  0.1309449156745228\n"
     ]
    }
   ],
   "source": [
    "#only german tweets\n",
    "df_ger = df.loc[df['lang'] == \"de\"]\n",
    "df_en = df.loc[df['lang'] == \"en\"]\n",
    "ger_model = perform_LDA(df_ger['lemmatized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17526cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Topic 0 ------\n",
      "work contract year system job research permanent many position phd\n",
      "\n",
      "------ Topic 1 ------\n",
      "year career researcher job contract position system know time research\n",
      "\n",
      "------ Topic 2 ------\n",
      "phd need contract system working condition change science english career\n",
      "\n",
      "------ Topic 3 ------\n",
      "#ichbinreyhan research system get like university want job working think\n",
      "\n",
      "------ Topic 4 ------\n",
      "thread people problem one contract @mahaelhissy system work scholar @kinofrau1\n",
      "\n",
      "\n",
      "Perplexity:  -7.905068907451213\n",
      "\n",
      "Coherence Score:  0.25655043078366013\n"
     ]
    }
   ],
   "source": [
    "#only english tweets\n",
    "en_model = perform_LDA(df_en['lemmatized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ee73f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get topics by month\n",
    "df_october = df.loc[(df['new_date'] > '2021-10-01 00:00:00') & (df['new_date'] <= '2021-10-31 23:59:59')]\n",
    "df_november = df.loc[(df['new_date'] > '2021-11-01 00:00:00') & (df['new_date'] <= '2021-11-30 23:59:59')]\n",
    "df_december = df.loc[(df['new_date'] > '2021-12-01 00:00:00') & (df['new_date'] <= '2021-12-31 23:59:59')]\n",
    "df_january = df.loc[(df['new_date'] > '2022-01-01 00:00:00') & (df['new_date'] <= '2022-01-31 23:59:59')]\n",
    "df_february = df.loc[(df['new_date'] > '2022-02-01 00:00:00') & (df['new_date'] <= '2022-02-28 23:59:59')]\n",
    "df_march = df.loc[(df['new_date'] > '2022-03-01 00:00:00') & (df['new_date'] <= '2022-03-31 23:59:59')]\n",
    "df_april = df.loc[(df['new_date'] > '2022-04-01 00:00:00') & (df['new_date'] <= '2022-04-30 23:59:59')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d0a562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dic_corpus(tokens):\n",
    "    dic = Dictionary(tokens)\n",
    "    dic.filter_extremes(no_below=2, no_above=.99)\n",
    "    corpus = [dic.doc2bow(d) for d in tokens]\n",
    "    return dic, corpus\n",
    "#get optimal number of topics for each (sub)set\n",
    "def compute_coherence_values_topics(tokens, limit=10, start=2, step=1):\n",
    "    dic, corpus = create_dic_corpus(tokens)\n",
    "    coherence_values_topic = []\n",
    "    model_list_topic = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = LdaMulticore(corpus=corpus, num_topics=num_topics, id2word=dic, random_state = 1)\n",
    "        model_list_topic.append(num_topics)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=tokens, dictionary=dic, coherence='c_v')\n",
    "        coherence_values_topic.append(coherencemodel.get_coherence())\n",
    "\n",
    "    print(model_list_topic, coherence_values_topic)\n",
    "    return(model_list_topic[np.argmax(coherence_values_topic)])\n",
    "\n",
    "    \n",
    "def compute_coherence_values_passes(tokens,num_topics):\n",
    "    \n",
    "    passes = [5,10,15,20]\n",
    "    dic, corpus = create_dic_corpus(tokens)\n",
    "    coherence_values_passes = []\n",
    "    model_list_passes = []\n",
    "    for num_pass in passes:\n",
    "        model = LdaMulticore(corpus=corpus, num_topics=num_topics, id2word=dic, passes = num_pass, random_state = 1)\n",
    "        model_list_passes.append(num_pass)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=tokens, dictionary=dic, coherence='c_v')\n",
    "        coherence_values_passes.append(coherencemodel.get_coherence())\n",
    "\n",
    "    print(model_list_passes, coherence_values_passes)\n",
    "    return(model_list_passes[np.argmax(coherence_values_passes)])\n",
    "    \n",
    "def compute_coherence_values_alpha(tokens,num_topics, passes):\n",
    "    \n",
    "    alpha = ['symmetric','asymmetric']\n",
    "    dic, corpus = create_dic_corpus(tokens)\n",
    "    coherence_values_alpha = []\n",
    "    model_list_alpha = []\n",
    "    for a in alpha:\n",
    "        model = LdaMulticore(corpus=corpus, num_topics=num_topics, id2word=dic, passes = passes, alpha = a, random_state = 1)\n",
    "        model_list_alpha.append(a)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=tokens, dictionary=dic, coherence='c_v')\n",
    "        coherence_values_alpha.append(coherencemodel.get_coherence())\n",
    "\n",
    "    print(model_list_alpha, coherence_values_alpha)\n",
    "    return(model_list_alpha[np.argmax(coherence_values_alpha)])\n",
    "    \n",
    "def compute_coherence_values_decay(tokens,num_topics, passes, alpha):\n",
    "    \n",
    "    decay = [0.5,0.7,0.9]\n",
    "    dic, corpus = create_dic_corpus(tokens)\n",
    "    coherence_values_decay = []\n",
    "    model_list_decay = []\n",
    "    for d in decay:\n",
    "        model = LdaMulticore(corpus=corpus, num_topics=num_topics, id2word=dic, passes = passes, alpha = alpha, random_state = 1, decay =d)\n",
    "        model_list_decay.append(d)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=tokens, dictionary=dic, coherence='c_v')\n",
    "        coherence_values_decay.append(coherencemodel.get_coherence())\n",
    "\n",
    "    print(model_list_decay, coherence_values_decay)\n",
    "    return(model_list_decay[np.argmax(coherence_values_decay)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd857f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4, 5, 6, 7, 8, 9] [0.20942709036718882, 0.17651272488865366, 0.1655239379031196, 0.16269701716464058, 0.177887186461377, 0.17355040300847424, 0.17759540943306415, 0.17767310316737922]\n",
      "[2, 3, 4, 5, 6, 7, 8, 9] [0.20150935623536642, 0.21219328479388752, 0.21923247396414478, 0.23322762215007592, 0.2587449125001096, 0.25356037620103095, 0.25229446678093814, 0.2651279236080119]\n",
      "[2, 3, 4, 5, 6, 7, 8, 9] [0.1538619294560588, 0.1257248811979674, 0.1316015833282439, 0.12465620584424042, 0.12075230258930002, 0.11393765349526275, 0.13004857168308487, 0.12974155717781655]\n",
      "[2, 3, 4, 5, 6, 7, 8, 9] [0.34813409349800056, 0.33175869755338877, 0.3393999594392588, 0.3618856901959886, 0.3646382056456497, 0.40035160713735074, 0.3742170053754307, 0.3610288628210986]\n",
      "[2, 3, 4, 5, 6, 7, 8, 9] [0.3527748286823005, 0.37532498520346574, 0.3806727961111532, 0.3832688745153251, 0.38527927412838686, 0.3835998691653932, 0.3949504019654041, 0.38474338748173936]\n",
      "[2, 3, 4, 5, 6, 7, 8, 9] [0.4897096372254163, 0.46808617880607223, 0.5064479782990079, 0.5053541927881334, 0.5134733447644446, 0.5172658695949368, 0.5159615558938095, 0.5033914107883242]\n",
      "[2, 3, 4, 5, 6, 7, 8, 9] [0.39056060921461694, 0.41427499941447526, 0.4132970315589709, 0.43105628329871, 0.4839319226640451, 0.48143929752805115, 0.47791316604807016, 0.4894977885517735]\n",
      "[2, 3, 4, 5, 6, 7, 8, 9] [0.5182065433907963, 0.5114610996100607, 0.5254242672459816, 0.5381057810958874, 0.5387596192229855, 0.5396565548088963, 0.5030738356006741, 0.5209818566888738]\n",
      "[2, 3, 4, 5, 6, 7, 8, 9] [0.41857270454027884, 0.4691317384986653, 0.47525336574265453, 0.49325680021365137, 0.5069168569306225, 0.4610472921796235, 0.5012448993616504, 0.49173559673778133]\n",
      "[2, 3, 4, 5, 6, 7, 8, 9] [0.5206488889258676, 0.5335488819804026, 0.5697767949806372, 0.5408977288033342, 0.5367347360923732, 0.5417424397316767, 0.5605305751647577, 0.5503454715687008]\n"
     ]
    }
   ],
   "source": [
    "#full data\n",
    "fullt = compute_coherence_values_topics(df['lemmatized'])\n",
    "#english data\n",
    "engt = compute_coherence_values_topics(df_en['lemmatized'])\n",
    "#german data\n",
    "gert = compute_coherence_values_topics(df_ger['lemmatized'])\n",
    "#october data\n",
    "octt = compute_coherence_values_topics(df_october['lemmatized'])\n",
    "#november data\n",
    "novt = compute_coherence_values_topics(df_november['lemmatized'])\n",
    "#december data\n",
    "dect = compute_coherence_values_topics(df_december['lemmatized'])\n",
    "#january data\n",
    "jant = compute_coherence_values_topics(df_january['lemmatized'])\n",
    "#february data\n",
    "febt = compute_coherence_values_topics(df_february['lemmatized'])\n",
    "#march data\n",
    "mart = compute_coherence_values_topics(df_march['lemmatized'])\n",
    "#april data\n",
    "aprt = compute_coherence_values_topics(df_april['lemmatized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "874d2889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 10, 15, 20] [0.24209267698760617, 0.21886209933366757, 0.21446740725109142, 0.24867186465156824]\n",
      "[5, 10, 15, 20] [0.26665668548613347, 0.2637055885624337, 0.25847943823749653, 0.2625805372072232]\n",
      "[5, 10, 15, 20] [0.12037027384113425, 0.12100757915656632, 0.1244809149019389, 0.1244809149019389]\n",
      "[5, 10, 15, 20] [0.43680018377580826, 0.4371549353278415, 0.4216693643727405, 0.4090508381269662]\n",
      "[5, 10, 15, 20] [0.4102189966438579, 0.41920294048258433, 0.4119814776753317, 0.40524465690989475]\n",
      "[5, 10, 15, 20] [0.5049158153286192, 0.5082212994658306, 0.5029604552557182, 0.5071165092166703]\n",
      "[5, 10, 15, 20] [0.47884093153356866, 0.4786545184823492, 0.47081712928189934, 0.4752796477111014]\n",
      "[5, 10, 15, 20] [0.5459866917983358, 0.5450343921586717, 0.5399708114323836, 0.5404115818073791]\n",
      "[5, 10, 15, 20] [0.48351872074825897, 0.4791349059072609, 0.4794033133238538, 0.47641226788728597]\n",
      "[5, 10, 15, 20] [0.5404205658584518, 0.5576526718070036, 0.5575939971765392, 0.553098294509926]\n"
     ]
    }
   ],
   "source": [
    "#full data\n",
    "fullp = compute_coherence_values_passes(df['lemmatized'], fullt)\n",
    "#english data\n",
    "engp = compute_coherence_values_passes(df_en['lemmatized'], engt)\n",
    "#german data\n",
    "gerp = compute_coherence_values_passes(df_ger['lemmatized'], gert)\n",
    "#october data\n",
    "octp = compute_coherence_values_passes(df_october['lemmatized'], octt)\n",
    "#november data\n",
    "novp = compute_coherence_values_passes(df_november['lemmatized'], novt)\n",
    "#december data\n",
    "decp = compute_coherence_values_passes(df_december['lemmatized'], dect)\n",
    "#january data\n",
    "janp = compute_coherence_values_passes(df_january['lemmatized'], jant)\n",
    "#february data\n",
    "febp = compute_coherence_values_passes(df_february['lemmatized'], febt)\n",
    "#march data\n",
    "marp = compute_coherence_values_passes(df_march['lemmatized'], mart)\n",
    "#april data\n",
    "aprp = compute_coherence_values_passes(df_april['lemmatized'], aprt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0dd4616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['symmetric', 'asymmetric'] [0.24867186465156824, 0.4371983661135381]\n",
      "['symmetric', 'asymmetric'] [0.26665668548613347, 0.2688917729439723]\n",
      "['symmetric', 'asymmetric'] [0.1244809149019389, 0.13803249796904893]\n",
      "['symmetric', 'asymmetric'] [0.4371549353278415, 0.4075015145199893]\n",
      "['symmetric', 'asymmetric'] [0.41920294048258433, 0.396584496213706]\n",
      "['symmetric', 'asymmetric'] [0.5082212994658306, 0.5182818582139231]\n",
      "['symmetric', 'asymmetric'] [0.47884093153356866, 0.4747444713513567]\n",
      "['symmetric', 'asymmetric'] [0.5459866917983358, 0.5213595036325166]\n",
      "['symmetric', 'asymmetric'] [0.48351872074825897, 0.4364870916584099]\n",
      "['symmetric', 'asymmetric'] [0.5576526718070036, 0.5548377352048373]\n"
     ]
    }
   ],
   "source": [
    "#full data\n",
    "fulla = compute_coherence_values_alpha(df['lemmatized'], fullt, fullp)\n",
    "#english data\n",
    "enga = compute_coherence_values_alpha(df_en['lemmatized'], engt, engp)\n",
    "#german data\n",
    "gera = compute_coherence_values_alpha(df_ger['lemmatized'], gert, gerp)\n",
    "#october data\n",
    "octa = compute_coherence_values_alpha(df_october['lemmatized'], octt, octp)\n",
    "#november data\n",
    "nova = compute_coherence_values_alpha(df_november['lemmatized'], novt, novp)\n",
    "#december data\n",
    "deca = compute_coherence_values_alpha(df_december['lemmatized'], dect, decp)\n",
    "#january data\n",
    "jana = compute_coherence_values_alpha(df_january['lemmatized'], jant, janp)\n",
    "#february data\n",
    "feba = compute_coherence_values_alpha(df_february['lemmatized'], febt, febp)\n",
    "#march data\n",
    "mara = compute_coherence_values_alpha(df_march['lemmatized'], mart, marp)\n",
    "#april data\n",
    "apra = compute_coherence_values_alpha(df_april['lemmatized'], aprt, aprp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "235b28c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5, 0.7, 0.9] [0.4371983661135381, 0.4425860936909295, 0.4309099557612427]\n",
      "[0.5, 0.7, 0.9] [0.2688917729439723, 0.26070712240425403, 0.26854282554060455]\n",
      "[0.5, 0.7, 0.9] [0.15675988826273551, 0.14641196770650064, 0.1478038609933699]\n",
      "[0.5, 0.7, 0.9] [0.4371549353278415, 0.4267502483912712, 0.4271102330369674]\n",
      "[0.5, 0.7, 0.9] [0.41920294048258433, 0.40903853881600305, 0.41116676649776485]\n",
      "[0.5, 0.7, 0.9] [0.5182818582139231, 0.514136916123627, 0.5138354994017582]\n",
      "[0.5, 0.7, 0.9] [0.47884093153356866, 0.47559833837451243, 0.47988035135457136]\n",
      "[0.5, 0.7, 0.9] [0.5459866917983358, 0.5459866917983358, 0.5521462850077123]\n",
      "[0.5, 0.7, 0.9] [0.48351872074825897, 0.48779333521485024, 0.4889898238668444]\n",
      "[0.5, 0.7, 0.9] [0.5576526718070036, 0.5522268017241497, 0.5513391534273439]\n"
     ]
    }
   ],
   "source": [
    "#full data\n",
    "fulld = compute_coherence_values_decay(df['lemmatized'], fullt, fullp, fulla)\n",
    "#english data\n",
    "engd = compute_coherence_values_decay(df_en['lemmatized'], engt, engp, enga)\n",
    "#german data\n",
    "gerd = compute_coherence_values_decay(df_ger['lemmatized'], gert, gerp, gera)\n",
    "#october data\n",
    "octd = compute_coherence_values_decay(df_october['lemmatized'], octt, octp, octa)\n",
    "#november data\n",
    "novd = compute_coherence_values_decay(df_november['lemmatized'], novt, novp, nova)\n",
    "#december data\n",
    "decd = compute_coherence_values_decay(df_december['lemmatized'], dect, decp, deca)\n",
    "#january data\n",
    "jand = compute_coherence_values_decay(df_january['lemmatized'], jant, janp, jana)\n",
    "#february data\n",
    "febd = compute_coherence_values_decay(df_february['lemmatized'], febt, febp, feba)\n",
    "#march data\n",
    "mard = compute_coherence_values_decay(df_march['lemmatized'], mart, marp, mara)\n",
    "#april data\n",
    "aprd = compute_coherence_values_decay(df_april['lemmatized'], aprt, aprp, apra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0778411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 20 asymmetric 0.7\n",
      "9 5 asymmetric 0.5\n",
      "2 15 asymmetric 0.5\n",
      "7 10 symmetric 0.5\n",
      "8 10 symmetric 0.5\n",
      "7 10 asymmetric 0.5\n",
      "9 5 symmetric 0.9\n",
      "7 5 symmetric 0.9\n",
      "6 5 symmetric 0.9\n",
      "4 10 symmetric 0.5\n"
     ]
    }
   ],
   "source": [
    "print(fullt,fullp,fulla,fulld)\n",
    "print(engt,engp,enga,engd)\n",
    "print(gert,gerp,gera,gerd)\n",
    "print(octt,octp,octa,octd)\n",
    "print(novt,novp,nova,novd)\n",
    "print(dect,decp,deca,decd)\n",
    "print(jant,janp,jana,jand)\n",
    "print(febt,febp,feba,febd)\n",
    "print(mart,marp,mara,mard)\n",
    "print(aprt,aprp,apra,aprd)\n",
    "#two topic for the full and the german model do not really make sense here, so they are manually updated to 6 each\n",
    "fullt = 6\n",
    "gert = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f79df1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Topic 0 ------\n",
      "#ichbinreyhan uni jahr forschung problem lehre stellen @gew_bund befristet hochschule\n",
      "\n",
      "------ Topic 1 ------\n",
      "system #ichbinreyhan contract job work research phd position one working\n",
      "\n",
      "------ Topic 2 ------\n",
      "#ichbinreyhan wissen thema prekär arbeit stellen uni diskussion wissenschaftlich #95vswisszeitvg\n",
      "\n",
      "------ Topic 3 ------\n",
      "#ichbinreyhan uhr #tvstud @gew_bund #dasgewinnenwir jahr @nga_wiss professur studierend tenure\n",
      "\n",
      "------ Topic 4 ------\n",
      "#ichbinreyhan frage #ichbinhannaat #ugnovelle deutschland stellen mensch trend @faznet via\n",
      "\n",
      "------ Topic 5 ------\n",
      "jahr arbeit befristet zeit stelle frage stellen vertrag wissen job\n",
      "\n",
      "\n",
      "Perplexity:  -8.812228789873192\n",
      "\n",
      "Coherence Score:  0.3353817904326009\n"
     ]
    }
   ],
   "source": [
    "full_model = perform_LDA(df['lemmatized'], fullt, fullp, fulla, fulld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9124a8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Topic 0 ------\n",
      "thread #ichbinreyhan job researcher position career need like many phd\n",
      "\n",
      "------ Topic 1 ------\n",
      "contract system #ichbinreyhan year work people phd academic get university\n",
      "\n",
      "------ Topic 2 ------\n",
      "research year system contract phd permanent position career one job\n",
      "\n",
      "------ Topic 3 ------\n",
      "@mahaelhissy movement condition working white event change @kinofrau1 scholar @akellergew\n",
      "\n",
      "------ Topic 4 ------\n",
      "working condition precarious university work good discussion going check better\n",
      "\n",
      "------ Topic 5 ------\n",
      "i'm time work research #ichbinreyhan still thanks need mean science\n",
      "\n",
      "------ Topic 6 ------\n",
      "work system researcher one need university time see change contract\n",
      "\n",
      "------ Topic 7 ------\n",
      "work career research one job working condition contract time #ichbinreyhan\n",
      "\n",
      "------ Topic 8 ------\n",
      "contract research science system education debate get people university job\n",
      "\n",
      "\n",
      "Perplexity:  -8.03977096219536\n",
      "\n",
      "Coherence Score:  0.2692967402593112\n"
     ]
    }
   ],
   "source": [
    "eng_model = perform_LDA(df_en['lemmatized'], engt, engp, enga, engd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef802ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Topic 0 ------\n",
      "#ichbinreyhan uni jahr arbeitsbedingung stelle wissen prekär vertrag hochschule @gew_bund\n",
      "\n",
      "------ Topic 1 ------\n",
      "deutschland problem prekär stellen jahr buch forschung befristet wissenschaftlich professur\n",
      "\n",
      "------ Topic 2 ------\n",
      "streitschrift #ichbinreyhan richtig jahr wissenschaftlich wichtig uni wissenschaftssystem monat schön\n",
      "\n",
      "------ Topic 3 ------\n",
      "frage universität #ichbinreyhan arbeit mensch system forschung publikation stellen leben\n",
      "\n",
      "------ Topic 4 ------\n",
      "#ichbinreyhan problem @jenniferhenkehb arbeit uni #frististfrust @nga_wiss #dauerstellenfürdaueraufgaben deutschland befristung\n",
      "\n",
      "------ Topic 5 ------\n",
      "#tvstud universität #lauterbachruecktritt uni @gew_bund wissenschaftssystem debatte #ichbinreyhan geld industrie\n",
      "\n",
      "\n",
      "Perplexity:  -8.945290741361394\n",
      "\n",
      "Coherence Score:  0.2256829226190252\n"
     ]
    }
   ],
   "source": [
    "ger_model = perform_LDA(df_ger['lemmatized'], gert, gerp, gera, gerd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b07cca87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Topic 0 ------\n",
      "#wisssystemfehler hochschule #dauerstell system sprechen @hrk_aktuell deutschland @gew_bund alt deutsch\n",
      "\n",
      "------ Topic 1 ------\n",
      "miller #hannasbegabung anlehnung universität stelle groß hochschule forschung prekär interessant\n",
      "\n",
      "------ Topic 2 ------\n",
      "#ichbinreyhan #wisssystemfehler jahr professur system uni lehre wissenschaftssystem deutsch forschung\n",
      "\n",
      "------ Topic 3 ------\n",
      "#wisssystemfehler #ichbinreyhan befristet professur problem vertrag arbeit richtig jahr mittelbau\n",
      "\n",
      "------ Topic 4 ------\n",
      "arbeit arbeitsbedingung @gew_bund #tvstud zeit uni diskutieren frage wissen #ichbinreyhan\n",
      "\n",
      "------ Topic 5 ------\n",
      "#tvstud beschäftigt jahr wissenschaftlich hamburg mitarbeiter uni #dasgewinnenwir #stopthecuts forschung\n",
      "\n",
      "------ Topic 6 ------\n",
      "uni system stellen jahr stelle #wisssystemfehler liebe problem richtig #ichbinreyhan\n",
      "\n",
      "\n",
      "Perplexity:  -8.11839404688689\n",
      "\n",
      "Coherence Score:  0.40434387403306976\n"
     ]
    }
   ],
   "source": [
    "oct_model = perform_LDA(df_october['lemmatized'], octt, octp, octa, octd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f2e027c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Topic 0 ------\n",
      "arbeitsbedingung @akellergew prekär #ichbinreyhan #koalitionsvertrag @gew_bund postdocs professur akademisch land\n",
      "\n",
      "------ Topic 1 ------\n",
      "system #ichbinreyhan jahr reform prekär zeit mittelbau monat postdoc thema\n",
      "\n",
      "------ Topic 2 ------\n",
      "#ichbinreyhan #tvstud #frististfrust #wisssystemfehler #wirhabenbedarf jahr beschäftigt @adressel arbeitsbedingung uni\n",
      "\n",
      "------ Topic 3 ------\n",
      "@anja_steinbeck #wisssystemfehler diskussion forschung stellen uni system karriere bildung @gew_bund\n",
      "\n",
      "------ Topic 4 ------\n",
      "#ichbinreyhan lehre uhr forschung #wisssystemfehler zeigen chance woche system problem\n",
      "\n",
      "------ Topic 5 ------\n",
      "@gew_bund #tvstud #dauerstell #dasgewinnenwir #keineausnahme deutschland hochschule #troed21 @ladybitchray1 #wisssystemfehler\n",
      "\n",
      "------ Topic 6 ------\n",
      "stellen universität #ichbinreyhan uni via frage #berlhg jahr @faznet berlin\n",
      "\n",
      "------ Topic 7 ------\n",
      "uhr #ichbinreyhan #tvstud forschung #dasgewinnenwir hochschule kolleg arbeit problem #warnstreik\n",
      "\n",
      "\n",
      "Perplexity:  -8.049025273514914\n",
      "\n",
      "Coherence Score:  0.40059619029432164\n"
     ]
    }
   ],
   "source": [
    "nov_model = perform_LDA(df_november['lemmatized'], novt, novp, nova, novd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d04d57a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Topic 0 ------\n",
      "#ichbinreyhan @hrk_aktuell jahr @gew_bund #hrkadvent türchen #adventskalender stellen lehre #wisssystemfehler\n",
      "\n",
      "------ Topic 1 ------\n",
      "uni forderung double-binds realität folge forschung jahr #ichbinreyhan befristet tag\n",
      "\n",
      "------ Topic 2 ------\n",
      "arbeit hochschule schön leute studium thread @_verdi gerne #ichbinreyhan befristet\n",
      "\n",
      "------ Topic 3 ------\n",
      "#ichbinreyhan liebe denken wissen research lehre @unibremen kosten arbeit hochschule\n",
      "\n",
      "------ Topic 4 ------\n",
      "@humboldtuni @bverfg kaputt #berlhg abschluss mensch @mliebendoerfer verpassen vorgang gehen\n",
      "\n",
      "------ Topic 5 ------\n",
      "podiumsdiskussion idee @fellercarsten @christine_blume @jule_specht entwickeln kübler andrea reden situation\n",
      "\n",
      "------ Topic 6 ------\n",
      "stellen liebe arbeit nachwuchs uni twitter gute stelle 2021 #ichbinhannaat\n",
      "\n",
      "\n",
      "Perplexity:  -7.716203525626036\n",
      "\n",
      "Coherence Score:  0.5277699483215332\n"
     ]
    }
   ],
   "source": [
    "dec_model = perform_LDA(df_december['lemmatized'], dect, decp, deca, decd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ceafa8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Topic 0 ------\n",
      "#ichbinreyhan deutsch lehre system @starkwatzinger forschung problem job studierend wissen\n",
      "\n",
      "------ Topic 1 ------\n",
      "sprechen #ichbinreyhan befristet stark system jahr uni problem @jenniferhenkehb krieg\n",
      "\n",
      "------ Topic 2 ------\n",
      "#ichbinreyhan @unileipzig @histodigitale @agehrlach @unv_nunftbegabt @mliebendoerfer thema @piczenik1 beitrag #wisssystemfehler\n",
      "\n",
      "------ Topic 3 ------\n",
      "#ichbinreyhan #wisssystemfehler akademisch problem thema zeit forschung #wissenschaft ändern warten\n",
      "\n",
      "------ Topic 4 ------\n",
      "uni #ichbinreyhan frage wissen zeit befristet system projekt @sainethina hochschule\n",
      "\n",
      "------ Topic 5 ------\n",
      "#ichbinreyhan jahr job arbeit stelle system uni vertrag mensch elternzeit\n",
      "\n",
      "------ Topic 6 ------\n",
      "stellen #ichbinreyhan problem arbeitsbedingung universität forschung befristet professur phd schuld\n",
      "\n",
      "------ Topic 7 ------\n",
      "jahr @starkwatzinger #ichbinreyhan @jenniferhenkehb @mliebendoerfer wissen führen stelle promotion glück\n",
      "\n",
      "------ Topic 8 ------\n",
      "jahr uni arbeit zeit #wisssystemfehler debatte wichtig frage kurz treffen\n",
      "\n",
      "\n",
      "Perplexity:  -8.137068137565207\n",
      "\n",
      "Coherence Score:  0.4766970879321673\n"
     ]
    }
   ],
   "source": [
    "jan_model = perform_LDA(df_january['lemmatized'], jant, janp, jana, jand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6bbe04b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Topic 0 ------\n",
      "#ichbinreyhan #wisssystemfehler stellen groß wissenschaftlich #hannaorganisiertsich #unigöttingen problem much @starkwatzinger\n",
      "\n",
      "------ Topic 1 ------\n",
      "#ichbinreyhan jahr wissen frage #firstgen professur stellen finanziell woche @gew_bund\n",
      "\n",
      "------ Topic 2 ------\n",
      "uni forschung #ichbinreyhan lehre gut arbeit jahr denken forschend minen\n",
      "\n",
      "------ Topic 3 ------\n",
      "#ichbinreyhan forschung #academicprecarity zeit #oneofusallofus problem system international hochschule akademisch\n",
      "\n",
      "------ Topic 4 ------\n",
      "#lenzen dieter #jlugiessen @jlugiessen uni herr arbeit jahr verstehen grund\n",
      "\n",
      "------ Topic 5 ------\n",
      "#ichbinreyhan @starkwatzinger befristung wichtig thema job liebe uni hochschule schaffen\n",
      "\n",
      "------ Topic 6 ------\n",
      "#wissenschaft #phdlife problem #ichbinreyhan aktuell zeit thema woche system befristet\n",
      "\n",
      "\n",
      "Perplexity:  -7.883955659696476\n",
      "\n",
      "Coherence Score:  0.5051161164454822\n"
     ]
    }
   ],
   "source": [
    "feb_model = perform_LDA(df_february['lemmatized'], febt, febp, feba, febd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9fbbdb5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Topic 0 ------\n",
      "@jenniferhenkehb @starkwatzinger arbeit stellen jahr #ichbinreyhan uni freuen bezahlen system\n",
      "\n",
      "------ Topic 1 ------\n",
      "#ichbinreyhan buch system prekär jahr deutschland #wisssystemfehler arbeit liebe forschung\n",
      "\n",
      "------ Topic 2 ------\n",
      "prekär #ichbinreyhan uni postdoc befristet frage forschung vertrag arbeit idiotisch\n",
      "\n",
      "------ Topic 3 ------\n",
      "buch @suhrkamp #ichbinreyhan vertrag prekär thema stelle arbeit deutschland herzlich\n",
      "\n",
      "------ Topic 4 ------\n",
      "forschung #ichbinreyhan #weilwirwissenschaftlieben #thesis_ev #mentalhealth deutschland #berlhg @gew_bund akademisch #dauerstell\n",
      "\n",
      "------ Topic 5 ------\n",
      "jahr befristet professur unbefristet promotion karriere stellen uni mensch wissenschaftlich\n",
      "\n",
      "\n",
      "Perplexity:  -7.7100296859833\n",
      "\n",
      "Coherence Score:  0.505062252260118\n"
     ]
    }
   ],
   "source": [
    "mar_model = perform_LDA(df_march['lemmatized'], mart, marp, mara, mard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c017b511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Topic 0 ------\n",
      "liebe @maithi_nk @drlutzboehm buch @michael_gerloff thema prekär @maithinkx deutschland richtig\n",
      "\n",
      "------ Topic 1 ------\n",
      "#ichbinreyhan streitschrift stellen forschung leute brotlose arbeit problem deutsch #unigöttingen\n",
      "\n",
      "------ Topic 2 ------\n",
      "#ichbinreyhan @maithinkx arbeitsbedingung @maithi_nk #wisssystemfehler jahr thema wissen wichtig problem\n",
      "\n",
      "------ Topic 3 ------\n",
      "wissenschaftlich @maithi_nk problem arbeitsbedingung wissenschaftssystem nachwuchs thread prof #lauterbachruecktrittjetzt endlich\n",
      "\n",
      "\n",
      "Perplexity:  -7.2436653824073245\n",
      "\n",
      "Coherence Score:  0.5609241104079168\n"
     ]
    }
   ],
   "source": [
    "apr_model = perform_LDA(df_april['lemmatized'], aprt, aprp, apra, aprd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621dbfa3",
   "metadata": {},
   "source": [
    "### Descriptive statistic about documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7d0cab97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model:\n",
      "Number of documents:31110 Minimum word count:0 Maximum word count:47 Mean word count:10.3401800064288\n",
      "Minimum character count:0 Maximum character count:557 Mean character count:99.59244615879139\n",
      "\n",
      "English model:\n",
      "Number of documents:3589 Minimum word count:0 Maximum word count:34 Mean word count:13.5881861242686\n",
      "Minimum character count:0 Maximum character count:364 Mean character count:109.81303984396767\n",
      "\n",
      "German model:\n",
      "Number of documents:24937 Minimum word count:0 Maximum word count:47 Mean word count:10.698119260536552\n",
      "Minimum character count:0 Maximum character count:557 Mean character count:105.89902554437182\n",
      "\n",
      "October model:\n",
      "Number of documents:2144 Minimum word count:0 Maximum word count:28 Mean word count:10.71455223880597\n",
      "Minimum character count:0 Maximum character count:341 Mean character count:106.37033582089552\n",
      "\n",
      "November model:\n",
      "Number of documents:1955 Minimum word count:0 Maximum word count:47 Mean word count:11.011764705882353\n",
      "Minimum character count:0 Maximum character count:557 Mean character count:109.39079283887467\n",
      "\n",
      "December model:\n",
      "Number of documents:1011 Minimum word count:0 Maximum word count:27 Mean word count:10.82492581602374\n",
      "Minimum character count:0 Maximum character count:275 Mean character count:105.83976261127596\n",
      "\n",
      "January model:\n",
      "Number of documents:1274 Minimum word count:0 Maximum word count:36 Mean word count:10.95054945054945\n",
      "Minimum character count:0 Maximum character count:278 Mean character count:106.19858712715856\n",
      "\n",
      "February model:\n",
      "Number of documents:1193 Minimum word count:0 Maximum word count:30 Mean word count:10.309304274937134\n",
      "Minimum character count:0 Maximum character count:317 Mean character count:100.25146689019279\n",
      "\n",
      "March model:\n",
      "Number of documents:964 Minimum word count:0 Maximum word count:35 Mean word count:10.629668049792532\n",
      "Minimum character count:0 Maximum character count:249 Mean character count:101.29564315352697\n",
      "\n",
      "April model:\n",
      "Number of documents:683 Minimum word count:0 Maximum word count:28 Mean word count:10.142020497803808\n",
      "Minimum character count:0 Maximum character count:267 Mean character count:98.61639824304538\n"
     ]
    }
   ],
   "source": [
    "print(\"Full model:\")\n",
    "print(\"Number of documents:\" + str(len(df)) +\" Minimum word count:\" + str(min(df['lemmatized'].map(len))) + \" Maximum word count:\" + str(max(df['lemmatized'].map(len))) + \" Mean word count:\" + str(df['lemmatized'].map(len).mean()))\n",
    "print(\"Minimum character count:\" + str(min(df['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Maximum character count:\" + str(max(df['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Mean character count:\" +str(df['lemmatized'].apply(lambda x: ' '.join(x)).map(len).mean()))\n",
    "\n",
    "print(\"\\nEnglish model:\")\n",
    "print(\"Number of documents:\" + str(len(df_en)) +\" Minimum word count:\" + str(min(df_en['lemmatized'].map(len))) + \" Maximum word count:\" + str(max(df_en['lemmatized'].map(len))) + \" Mean word count:\" + str(df_en['lemmatized'].map(len).mean()))\n",
    "print(\"Minimum character count:\" + str(min(df_en['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Maximum character count:\" + str(max(df_en['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Mean character count:\" +str(df_en['lemmatized'].apply(lambda x: ' '.join(x)).map(len).mean()))\n",
    "\n",
    "print(\"\\nGerman model:\")\n",
    "print(\"Number of documents:\" + str(len(df_ger)) +\" Minimum word count:\" + str(min(df_ger['lemmatized'].map(len))) + \" Maximum word count:\" + str(max(df_ger['lemmatized'].map(len))) + \" Mean word count:\" + str(df_ger['lemmatized'].map(len).mean()))\n",
    "print(\"Minimum character count:\" + str(min(df_ger['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Maximum character count:\" + str(max(df_ger['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Mean character count:\" +str(df_ger['lemmatized'].apply(lambda x: ' '.join(x)).map(len).mean()))\n",
    "\n",
    "print(\"\\nOctober model:\")\n",
    "print(\"Number of documents:\" + str(len(df_october)) +\" Minimum word count:\" + str(min(df_october['lemmatized'].map(len))) + \" Maximum word count:\" + str(max(df_october['lemmatized'].map(len))) + \" Mean word count:\" + str(df_october['lemmatized'].map(len).mean()))\n",
    "print(\"Minimum character count:\" + str(min(df_october['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Maximum character count:\" + str(max(df_october['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Mean character count:\" +str(df_october['lemmatized'].apply(lambda x: ' '.join(x)).map(len).mean()))\n",
    "\n",
    "print(\"\\nNovember model:\")\n",
    "print(\"Number of documents:\" + str(len(df_november)) +\" Minimum word count:\" + str(min(df_november['lemmatized'].map(len))) + \" Maximum word count:\" + str(max(df_november['lemmatized'].map(len))) + \" Mean word count:\" + str(df_november['lemmatized'].map(len).mean()))\n",
    "print(\"Minimum character count:\" + str(min(df_november['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Maximum character count:\" + str(max(df_november['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Mean character count:\" +str(df_november['lemmatized'].apply(lambda x: ' '.join(x)).map(len).mean()))\n",
    "\n",
    "print(\"\\nDecember model:\")\n",
    "print(\"Number of documents:\" + str(len(df_december)) +\" Minimum word count:\" + str(min(df_december['lemmatized'].map(len))) + \" Maximum word count:\" + str(max(df_december['lemmatized'].map(len))) + \" Mean word count:\" + str(df_december['lemmatized'].map(len).mean()))\n",
    "print(\"Minimum character count:\" + str(min(df_december['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Maximum character count:\" + str(max(df_december['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Mean character count:\" +str(df_december['lemmatized'].apply(lambda x: ' '.join(x)).map(len).mean()))\n",
    "\n",
    "print(\"\\nJanuary model:\")\n",
    "print(\"Number of documents:\" + str(len(df_january)) +\" Minimum word count:\" + str(min(df_january['lemmatized'].map(len))) + \" Maximum word count:\" + str(max(df_january['lemmatized'].map(len))) + \" Mean word count:\" + str(df_january['lemmatized'].map(len).mean()))\n",
    "print(\"Minimum character count:\" + str(min(df_january['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Maximum character count:\" + str(max(df_january['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Mean character count:\" +str(df_january['lemmatized'].apply(lambda x: ' '.join(x)).map(len).mean()))\n",
    "\n",
    "print(\"\\nFebruary model:\")\n",
    "print(\"Number of documents:\" + str(len(df_february)) +\" Minimum word count:\" + str(min(df_february['lemmatized'].map(len))) + \" Maximum word count:\" + str(max(df_february['lemmatized'].map(len))) + \" Mean word count:\" + str(df_february['lemmatized'].map(len).mean()))\n",
    "print(\"Minimum character count:\" + str(min(df_february['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Maximum character count:\" + str(max(df_february['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Mean character count:\" +str(df_february['lemmatized'].apply(lambda x: ' '.join(x)).map(len).mean()))\n",
    "\n",
    "print(\"\\nMarch model:\")\n",
    "print(\"Number of documents:\" + str(len(df_march)) +\" Minimum word count:\" + str(min(df_march['lemmatized'].map(len))) + \" Maximum word count:\" + str(max(df_march['lemmatized'].map(len))) + \" Mean word count:\" + str(df_march['lemmatized'].map(len).mean()))\n",
    "print(\"Minimum character count:\" + str(min(df_march['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Maximum character count:\" + str(max(df_march['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Mean character count:\" +str(df_march['lemmatized'].apply(lambda x: ' '.join(x)).map(len).mean()))\n",
    "\n",
    "print(\"\\nApril model:\")\n",
    "print(\"Number of documents:\" + str(len(df_april)) +\" Minimum word count:\" + str(min(df_april['lemmatized'].map(len))) + \" Maximum word count:\" + str(max(df_april['lemmatized'].map(len))) + \" Mean word count:\" + str(df_april['lemmatized'].map(len).mean()))\n",
    "print(\"Minimum character count:\" + str(min(df_april['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Maximum character count:\" + str(max(df_april['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Mean character count:\" +str(df_april['lemmatized'].apply(lambda x: ' '.join(x)).map(len).mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0f7b78",
   "metadata": {},
   "source": [
    "## Pooling tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4dca3f",
   "metadata": {},
   "source": [
    "As tweets with 280 are very short documents especially for the approach of LDA. To overcome this problem, the following part explores different pooling strategies. This means that tweets that are somehow related will be concatenated to one document. The assumption here is that all people that tweet under the hashtag talk about very similar topics on the same day. Former analysis showed that conversation is very event-driven (e.g. #HannahImBundestag) and topics change over time, so this assumption is reasonable. For that reason and also to keep the temporal information, tweets are always at least aggregated on days. The first pooling strategy will only pool by day. It is expected that for the monthly models at least this will most likely lead to worse performance, because there will be too few documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e94a9f",
   "metadata": {},
   "source": [
    "### Pool tweets by day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "24f5dc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to pool by day the date format is changed to not include daytime\n",
    "df['date'] =  pd.to_datetime(df['new_date']).dt.strftime('%Y-%m-%d')\n",
    "df_en['date'] =  pd.to_datetime(df_en['new_date']).dt.strftime('%Y-%m-%d')\n",
    "df_ger['date'] =  pd.to_datetime(df_ger['new_date']).dt.strftime('%Y-%m-%d')\n",
    "df_october['date'] =  pd.to_datetime(df_october['new_date']).dt.strftime('%Y-%m-%d')\n",
    "df_november['date'] =  pd.to_datetime(df_november['new_date']).dt.strftime('%Y-%m-%d')\n",
    "df_december['date'] =  pd.to_datetime(df_december['new_date']).dt.strftime('%Y-%m-%d')\n",
    "df_january['date'] =  pd.to_datetime(df_january['new_date']).dt.strftime('%Y-%m-%d')\n",
    "df_february['date'] =  pd.to_datetime(df_february['new_date']).dt.strftime('%Y-%m-%d')\n",
    "df_march['date'] =  pd.to_datetime(df_march['new_date']).dt.strftime('%Y-%m-%d')\n",
    "df_april['date'] =  pd.to_datetime(df_april['new_date']).dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f716dac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p = df.groupby(['date'], as_index=False)['lemmatized'].sum()\n",
    "df_en_p = df_en.groupby(['date'], as_index=False)['lemmatized'].sum()\n",
    "df_ger_p = df_ger.groupby(['date'], as_index=False)['lemmatized'].sum()\n",
    "df_october_p = df_october.groupby(['date'], as_index=False)['lemmatized'].sum()\n",
    "df_november_p = df_november.groupby(['date'], as_index=False)['lemmatized'].sum()\n",
    "df_december_p = df_december.groupby(['date'], as_index=False)['lemmatized'].sum()\n",
    "df_january_p = df_january.groupby(['date'], as_index=False)['lemmatized'].sum()\n",
    "df_february_p = df_february.groupby(['date'], as_index=False)['lemmatized'].sum()\n",
    "df_march_p = df_march.groupby(['date'], as_index=False)['lemmatized'].sum()\n",
    "df_april_p = df_april.groupby(['date'], as_index=False)['lemmatized'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3ec81a",
   "metadata": {},
   "source": [
    "### Descriptive statistic about documents pooled by day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "bf2c5e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model:\n",
      "Number of documents:308 Minimum word count:37 Maximum word count:21826 Mean word count:1044.4253246753246\n",
      "Minimum character count:356 Maximum character count:204192 Mean character count:10155.584415584415\n",
      "\n",
      "English model:\n",
      "Number of documents:298 Minimum word count:1 Maximum word count:4081 Mean word count:163.6510067114094\n",
      "Minimum character count:8 Maximum character count:32069 Mean character count:1333.4194630872482\n",
      "\n",
      "German model:\n",
      "Number of documents:308 Minimum word count:37 Maximum word count:17926 Mean word count:866.1655844155844\n",
      "Minimum character count:356 Maximum character count:170394 Mean character count:8653.66883116883\n",
      "\n",
      "October model:\n",
      "Number of documents:31 Minimum word count:248 Maximum word count:1667 Mean word count:741.0322580645161\n",
      "Minimum character count:2483 Maximum character count:16793 Mean character count:7422.967741935484\n",
      "\n",
      "November model:\n",
      "Number of documents:30 Minimum word count:135 Maximum word count:2183 Mean word count:717.6\n",
      "Minimum character count:1334 Maximum character count:22191 Mean character count:7191.833333333333\n",
      "\n",
      "December model:\n",
      "Number of documents:31 Minimum word count:37 Maximum word count:739 Mean word count:353.03225806451616\n",
      "Minimum character count:356 Maximum character count:7086 Mean character count:3482.5806451612902\n",
      "\n",
      "January model:\n",
      "Number of documents:31 Minimum word count:147 Maximum word count:746 Mean word count:450.03225806451616\n",
      "Minimum character count:1397 Maximum character count:7493 Mean character count:4403.612903225807\n",
      "\n",
      "February model:\n",
      "Number of documents:28 Minimum word count:121 Maximum word count:30 Mean word count:439.25\n",
      "Minimum character count:1035 Maximum character count:9333 Mean character count:4312.0\n",
      "\n",
      "March model:\n",
      "Number of documents:31 Minimum word count:130 Maximum word count:35 Mean word count:330.5483870967742\n",
      "Minimum character count:1168 Maximum character count:5606 Mean character count:3179.548387096774\n",
      "\n",
      "April model:\n",
      "Number of documents:13 Minimum word count:151 Maximum word count:28 Mean word count:532.8461538461538\n",
      "Minimum character count:1407 Maximum character count:9109 Mean character count:5231.153846153846\n"
     ]
    }
   ],
   "source": [
    "print(\"Full model:\")\n",
    "print(\"Number of documents:\" + str(len(df_p)) +\" Minimum word count:\" + str(min(df_p['lemmatized'].map(len))) + \" Maximum word count:\" + str(max(df_p['lemmatized'].map(len))) + \" Mean word count:\" + str(df_p['lemmatized'].map(len).mean()))\n",
    "print(\"Minimum character count:\" + str(min(df_p['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Maximum character count:\" + str(max(df_p['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Mean character count:\" +str(df_p['lemmatized'].apply(lambda x: ' '.join(x)).map(len).mean()))\n",
    "\n",
    "print(\"\\nEnglish model:\")\n",
    "print(\"Number of documents:\" + str(len(df_en_p)) +\" Minimum word count:\" + str(min(df_en_p['lemmatized'].map(len))) + \" Maximum word count:\" + str(max(df_en_p['lemmatized'].map(len))) + \" Mean word count:\" + str(df_en_p['lemmatized'].map(len).mean()))\n",
    "print(\"Minimum character count:\" + str(min(df_en_p['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Maximum character count:\" + str(max(df_en_p['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Mean character count:\" +str(df_en_p['lemmatized'].apply(lambda x: ' '.join(x)).map(len).mean()))\n",
    "\n",
    "print(\"\\nGerman model:\")\n",
    "print(\"Number of documents:\" + str(len(df_ger_p)) +\" Minimum word count:\" + str(min(df_ger_p['lemmatized'].map(len))) + \" Maximum word count:\" + str(max(df_ger_p['lemmatized'].map(len))) + \" Mean word count:\" + str(df_ger_p['lemmatized'].map(len).mean()))\n",
    "print(\"Minimum character count:\" + str(min(df_ger_p['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Maximum character count:\" + str(max(df_ger_p['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Mean character count:\" +str(df_ger_p['lemmatized'].apply(lambda x: ' '.join(x)).map(len).mean()))\n",
    "\n",
    "print(\"\\nOctober model:\")\n",
    "print(\"Number of documents:\" + str(len(df_october_p)) +\" Minimum word count:\" + str(min(df_october_p['lemmatized'].map(len))) + \" Maximum word count:\" + str(max(df_october_p['lemmatized'].map(len))) + \" Mean word count:\" + str(df_october_p['lemmatized'].map(len).mean()))\n",
    "print(\"Minimum character count:\" + str(min(df_october_p['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Maximum character count:\" + str(max(df_october_p['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Mean character count:\" +str(df_october_p['lemmatized'].apply(lambda x: ' '.join(x)).map(len).mean()))\n",
    "\n",
    "print(\"\\nNovember model:\")\n",
    "print(\"Number of documents:\" + str(len(df_november_p)) +\" Minimum word count:\" + str(min(df_november_p['lemmatized'].map(len))) + \" Maximum word count:\" + str(max(df_november_p['lemmatized'].map(len))) + \" Mean word count:\" + str(df_november_p['lemmatized'].map(len).mean()))\n",
    "print(\"Minimum character count:\" + str(min(df_november_p['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Maximum character count:\" + str(max(df_november_p['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Mean character count:\" +str(df_november_p['lemmatized'].apply(lambda x: ' '.join(x)).map(len).mean()))\n",
    "\n",
    "print(\"\\nDecember model:\")\n",
    "print(\"Number of documents:\" + str(len(df_december_p)) +\" Minimum word count:\" + str(min(df_december_p['lemmatized'].map(len))) + \" Maximum word count:\" + str(max(df_december_p['lemmatized'].map(len))) + \" Mean word count:\" + str(df_december_p['lemmatized'].map(len).mean()))\n",
    "print(\"Minimum character count:\" + str(min(df_december_p['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Maximum character count:\" + str(max(df_december_p['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Mean character count:\" +str(df_december_p['lemmatized'].apply(lambda x: ' '.join(x)).map(len).mean()))\n",
    "\n",
    "print(\"\\nJanuary model:\")\n",
    "print(\"Number of documents:\" + str(len(df_january_p)) +\" Minimum word count:\" + str(min(df_january_p['lemmatized'].map(len))) + \" Maximum word count:\" + str(max(df_january_p['lemmatized'].map(len))) + \" Mean word count:\" + str(df_january_p['lemmatized'].map(len).mean()))\n",
    "print(\"Minimum character count:\" + str(min(df_january_p['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Maximum character count:\" + str(max(df_january_p['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Mean character count:\" +str(df_january_p['lemmatized'].apply(lambda x: ' '.join(x)).map(len).mean()))\n",
    "\n",
    "print(\"\\nFebruary model:\")\n",
    "print(\"Number of documents:\" + str(len(df_february_p)) +\" Minimum word count:\" + str(min(df_february_p['lemmatized'].map(len))) + \" Maximum word count:\" + str(max(df_february['lemmatized'].map(len))) + \" Mean word count:\" + str(df_february_p['lemmatized'].map(len).mean()))\n",
    "print(\"Minimum character count:\" + str(min(df_february_p['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Maximum character count:\" + str(max(df_february_p['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Mean character count:\" +str(df_february_p['lemmatized'].apply(lambda x: ' '.join(x)).map(len).mean()))\n",
    "\n",
    "print(\"\\nMarch model:\")\n",
    "print(\"Number of documents:\" + str(len(df_march_p)) +\" Minimum word count:\" + str(min(df_march_p['lemmatized'].map(len))) + \" Maximum word count:\" + str(max(df_march['lemmatized'].map(len))) + \" Mean word count:\" + str(df_march_p['lemmatized'].map(len).mean()))\n",
    "print(\"Minimum character count:\" + str(min(df_march_p['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Maximum character count:\" + str(max(df_march_p['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Mean character count:\" +str(df_march_p['lemmatized'].apply(lambda x: ' '.join(x)).map(len).mean()))\n",
    "\n",
    "print(\"\\nApril model:\")\n",
    "print(\"Number of documents:\" + str(len(df_april_p)) +\" Minimum word count:\" + str(min(df_april_p['lemmatized'].map(len))) + \" Maximum word count:\" + str(max(df_april['lemmatized'].map(len))) + \" Mean word count:\" + str(df_april_p['lemmatized'].map(len).mean()))\n",
    "print(\"Minimum character count:\" + str(min(df_april_p['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Maximum character count:\" + str(max(df_april_p['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Mean character count:\" +str(df_april_p['lemmatized'].apply(lambda x: ' '.join(x)).map(len).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0370ad38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4, 5, 6, 7, 8, 9] [0.26460865667191613, 0.2635516829916517, 0.2630809197235411, 0.2652905169717595, 0.26710975636057843, 0.26533238180236685, 0.2651216603586509, 0.26525735016699037]\n",
      "[2, 3, 4, 5, 6, 7, 8, 9] [0.3005328299923204, 0.3092932345075032, 0.30012160890821654, 0.2967215288476258, 0.2955122385517411, 0.3011540265377396, 0.2959008585099691, 0.29855344572735143]\n",
      "[2, 3, 4, 5, 6, 7, 8, 9] [0.2398956848411093, 0.23831281432044324, 0.23977368050371461, 0.23807199219061054, 0.2370149770019786, 0.23744632867756152, 0.23905617096407816, 0.23766316285795813]\n",
      "[2, 3, 4, 5, 6, 7, 8, 9] [0.2182550014210474, 0.2248031600347641, 0.2274131532445296, 0.22762027236435797, 0.22781382703226363, 0.22584349623214345, 0.22718007721447314, 0.22874017432679888]\n",
      "[2, 3, 4, 5, 6, 7, 8, 9] [0.2114277965476843, 0.21726763762240822, 0.22206505311354569, 0.21738136680463233, 0.21424393885491047, 0.20860576710910725, 0.21637194421354172, 0.207346226980305]\n",
      "[2, 3, 4, 5, 6, 7, 8, 9] [0.2560250640824412, 0.24002895463353657, 0.2332939896490495, 0.23704879721572542, 0.24328500147379442, 0.24332493111113207, 0.23511684707537667, 0.23989749524941917]\n",
      "[2, 3, 4, 5, 6, 7, 8, 9] [0.2509489102661052, 0.2660956466090781, 0.2676498721657745, 0.264454159064461, 0.26233921678607824, 0.25887610315981663, 0.2600920603662124, 0.25921190228535534]\n",
      "[2, 3, 4, 5, 6, 7, 8, 9] [0.2560250640824412, 0.24002895463353657, 0.2332939896490495, 0.23704879721572542, 0.24328500147379442, 0.24332493111113207, 0.23511684707537667, 0.23989749524941917]\n",
      "[2, 3, 4, 5, 6, 7, 8, 9] [0.2657218991208322, 0.2745634433423606, 0.2697995073331284, 0.27120006082081627, 0.27495338308632733, 0.2741793581531042, 0.28194037789348353, 0.27923133232658204]\n",
      "[2, 3, 4, 5, 6, 7, 8, 9] [0.24362560564427665, 0.24495305097742223, 0.23816905152588458, 0.23179911440312123, 0.2478628522395491, 0.249447568457218, 0.2488893442762315, 0.2460786734504962]\n",
      "[5, 10, 15, 20] [0.28180882785395533, 0.2858918454390059, 0.286757241302369, 0.2867127931818515]\n",
      "[5, 10, 15, 20] [0.31003375437868125, 0.3000709993297026, 0.3000709993297026, 0.2912317632774108]\n",
      "[5, 10, 15, 20] [0.24002250015981197, 0.2506820307127172, 0.2506820307127172, 0.25360222674365357]\n",
      "[5, 10, 15, 20] [0.28355829699247714, 0.2821952975567672, 0.2833619332614736, 0.284519564451432]\n",
      "[5, 10, 15, 20] [0.2438585214068876, 0.26347973002993175, 0.26889774851605314, 0.27987572950053596]\n",
      "[5, 10, 15, 20] [0.25237036021982057, 0.2523703602198206, 0.2523703602198206, 0.2471287274848316]\n",
      "[5, 10, 15, 20] [0.2465065336555654, 0.2393156688782036, 0.25397565634445257, 0.24232422454299585]\n",
      "[5, 10, 15, 20] [0.25237036021982057, 0.2523703602198206, 0.2523703602198206, 0.2471287274848316]\n",
      "[5, 10, 15, 20] [0.27839993332477375, 0.2935548324455714, 0.2970721891895958, 0.2998993447348138]\n",
      "[5, 10, 15, 20] [0.2973752899670125, 0.2988024973008672, 0.29857872534252067, 0.30824152649439973]\n",
      "['symmetric', 'asymmetric'] [0.286757241302369, 0.2834310946747038]\n",
      "['symmetric', 'asymmetric'] [0.31003375437868125, 0.31646880421118007]\n",
      "['symmetric', 'asymmetric'] [0.25360222674365357, 0.2506820307127173]\n",
      "['symmetric', 'asymmetric'] [0.284519564451432, 0.3012676744155994]\n",
      "['symmetric', 'asymmetric'] [0.27987572950053596, 0.25014581493467564]\n",
      "['symmetric', 'asymmetric'] [0.25237036021982057, 0.2356336301967811]\n",
      "['symmetric', 'asymmetric'] [0.25397565634445257, 0.24392561335171808]\n",
      "['symmetric', 'asymmetric'] [0.25237036021982057, 0.2356336301967811]\n",
      "['symmetric', 'asymmetric'] [0.2998993447348138, 0.2911074249464579]\n",
      "['symmetric', 'asymmetric'] [0.30824152649439973, 0.30832452501042634]\n",
      "[0.5, 0.7, 0.9] [0.286757241302369, 0.2855510968354017, 0.28245165231200686]\n",
      "[0.5, 0.7, 0.9] [0.31646880421118007, 0.31259892017581076, 0.31406118367572966]\n",
      "[0.5, 0.7, 0.9] [0.25360222674365357, 0.2506820307127172, 0.2506820307127173]\n",
      "[0.5, 0.7, 0.9] [0.3012676744155994, 0.29717448856550654, 0.28789348981849927]\n",
      "[0.5, 0.7, 0.9] [0.27987572950053596, 0.2683641837441012, 0.2681162054118583]\n",
      "[0.5, 0.7, 0.9] [0.25237036021982057, 0.2523703602198206, 0.24105322930843057]\n",
      "[0.5, 0.7, 0.9] [0.25397565634445257, 0.2476653802084735, 0.25233794120911357]\n",
      "[0.5, 0.7, 0.9] [0.25237036021982057, 0.2523703602198206, 0.24105322930843057]\n",
      "[0.5, 0.7, 0.9] [0.2998993447348138, 0.28899831858606145, 0.28544410137738546]\n",
      "[0.5, 0.7, 0.9] [0.30832452501042634, 0.30902150928704153, 0.31538708308790137]\n"
     ]
    }
   ],
   "source": [
    "#full data\n",
    "fullt_p = compute_coherence_values_topics(df_p['lemmatized'])\n",
    "#english data\n",
    "engt_p = compute_coherence_values_topics(df_en_p['lemmatized'])\n",
    "#german data\n",
    "gert_p = compute_coherence_values_topics(df_ger_p['lemmatized'])\n",
    "#october data\n",
    "octt_p = compute_coherence_values_topics(df_october_p['lemmatized'])\n",
    "#november data\n",
    "novt_p = compute_coherence_values_topics(df_november_p['lemmatized'])\n",
    "#december data\n",
    "dect_p = compute_coherence_values_topics(df_december_p['lemmatized'])\n",
    "#january data\n",
    "jant_p = compute_coherence_values_topics(df_january_p['lemmatized'])\n",
    "#february data\n",
    "febt_p = compute_coherence_values_topics(df_february_p['lemmatized'])\n",
    "#march data\n",
    "mart_p = compute_coherence_values_topics(df_march_p['lemmatized'])\n",
    "#april data\n",
    "aprt_p = compute_coherence_values_topics(df_april_p['lemmatized'])\n",
    "\n",
    "#full data\n",
    "fullp_p = compute_coherence_values_passes(df_p['lemmatized'], fullt_p)\n",
    "#english data\n",
    "engp_p = compute_coherence_values_passes(df_en_p['lemmatized'], engt_p)\n",
    "#german data\n",
    "gerp_p = compute_coherence_values_passes(df_ger_p['lemmatized'], gert_p)\n",
    "#october data\n",
    "octp_p = compute_coherence_values_passes(df_october_p['lemmatized'], octt_p)\n",
    "#november data\n",
    "novp_p = compute_coherence_values_passes(df_november_p['lemmatized'], novt_p)\n",
    "#december data\n",
    "decp_p = compute_coherence_values_passes(df_december_p['lemmatized'], dect_p)\n",
    "#january data\n",
    "janp_p = compute_coherence_values_passes(df_january_p['lemmatized'], jant_p)\n",
    "#february data\n",
    "febp_p = compute_coherence_values_passes(df_february_p['lemmatized'], febt_p)\n",
    "#march data\n",
    "marp_p = compute_coherence_values_passes(df_march_p['lemmatized'], mart_p)\n",
    "#april data\n",
    "aprp_p = compute_coherence_values_passes(df_april_p['lemmatized'], aprt_p)\n",
    "\n",
    "#full data\n",
    "fulla_p = compute_coherence_values_alpha(df_p['lemmatized'], fullt_p, fullp_p)\n",
    "#english data\n",
    "enga_p = compute_coherence_values_alpha(df_en_p['lemmatized'], engt_p, engp_p)\n",
    "#german data\n",
    "gera_p = compute_coherence_values_alpha(df_ger_p['lemmatized'], gert_p, gerp_p)\n",
    "#october data\n",
    "octa_p = compute_coherence_values_alpha(df_october_p['lemmatized'], octt_p, octp_p)\n",
    "#november data\n",
    "nova_p = compute_coherence_values_alpha(df_november_p['lemmatized'], novt_p, novp_p)\n",
    "#december data\n",
    "deca_p = compute_coherence_values_alpha(df_december_p['lemmatized'], dect_p, decp_p)\n",
    "#january data\n",
    "jana_p = compute_coherence_values_alpha(df_january_p['lemmatized'], jant_p, janp_p)\n",
    "#february data\n",
    "feba_p = compute_coherence_values_alpha(df_february_p['lemmatized'], febt_p, febp_p)\n",
    "#march data\n",
    "mara_p = compute_coherence_values_alpha(df_march_p['lemmatized'], mart_p, marp_p)\n",
    "#april data\n",
    "apra_p = compute_coherence_values_alpha(df_april_p['lemmatized'], aprt_p, aprp_p)\n",
    "\n",
    "#full data\n",
    "fulld_p = compute_coherence_values_decay(df_p['lemmatized'], fullt_p, fullp_p, fulla_p)\n",
    "#english data\n",
    "engd_p = compute_coherence_values_decay(df_en_p['lemmatized'], engt_p, engp_p, enga_p)\n",
    "#german data\n",
    "gerd_p = compute_coherence_values_decay(df_ger_p['lemmatized'], gert_p, gerp_p, gera_p)\n",
    "#october data\n",
    "octd_p = compute_coherence_values_decay(df_october_p['lemmatized'], octt_p, octp_p, octa_p)\n",
    "#november data\n",
    "novd_p = compute_coherence_values_decay(df_november_p['lemmatized'], novt_p, novp_p, nova_p)\n",
    "#december data\n",
    "decd_p = compute_coherence_values_decay(df_december_p['lemmatized'], dect_p, decp_p, deca_p)\n",
    "#january data\n",
    "jand_p = compute_coherence_values_decay(df_january_p['lemmatized'], jant_p, janp_p, jana_p)\n",
    "#february data\n",
    "febd_p = compute_coherence_values_decay(df_february_p['lemmatized'], febt_p, febp_p, feba_p)\n",
    "#march data\n",
    "mard_p = compute_coherence_values_decay(df_march_p['lemmatized'], mart_p, marp_p, mara_p)\n",
    "#april data\n",
    "aprd_p = compute_coherence_values_decay(df_april_p['lemmatized'], aprt_p, aprp_p, apra_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "76ba9c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 15 symmetric 0.5\n",
      "6 5 asymmetric 0.5\n",
      "6 20 symmetric 0.5\n",
      "9 20 asymmetric 0.5\n",
      "4 20 symmetric 0.5\n",
      "8 20 symmetric 0.5\n",
      "4 15 symmetric 0.5\n",
      "4 10 symmetric 0.7\n",
      "8 20 symmetric 0.5\n",
      "7 20 asymmetric 0.9\n"
     ]
    }
   ],
   "source": [
    "print(fullt_p,fullp_p,fulla_p,fulld_p)\n",
    "print(engt_p,engp_p,enga_p,engd_p)\n",
    "print(gert_p,gerp_p,gera_p,gerd_p)\n",
    "print(octt_p,octp_p,octa_p,octd_p)\n",
    "print(novt_p,novp_p,nova_p,novd_p)\n",
    "print(dect_p,decp_p,deca_p,decd_p)\n",
    "print(jant_p,janp_p,jana_p,jand_p)\n",
    "print(febt_p,febp_p,feba_p,febd_p)\n",
    "print(mart_p,marp_p,mara_p,mard_p)\n",
    "print(aprt_p,aprp_p,apra_p,aprd_p)\n",
    "#manually increase number of topics\n",
    "engt_p = 6\n",
    "gert_p = 6\n",
    "febt_p = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "bdadc0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Topic 0 ------\n",
      "#ichbinreyhan jahr uni stellen zeit prekär @gew_bund system arbeit problem\n",
      "\n",
      "------ Topic 1 ------\n",
      "#ichbinreyhan uni jahr stellen system @gew_bund problem arbeit forschung #wisssystemfehler\n",
      "\n",
      "------ Topic 2 ------\n",
      "#ichbinreyhan uni jahr vertrag @gew_bund stellen arbeit forschung hochschule system\n",
      "\n",
      "------ Topic 3 ------\n",
      "#ichbinreyhan system jahr forschung problem karriere uni zeit stelle wissen\n",
      "\n",
      "------ Topic 4 ------\n",
      "#hannaimbundestag @anjakarliczek jahr hochschule bundestag uni aktuell problem forschung arbeitsbedingung\n",
      "\n",
      "------ Topic 5 ------\n",
      "jahr uni befristet system forschung @anjakarliczek problem arbeit stellen vertrag\n",
      "\n",
      "\n",
      "Perplexity:  -8.733679674173711\n",
      "\n",
      "Coherence Score:  0.2946533306225232\n"
     ]
    }
   ],
   "source": [
    "full_model_p = perform_LDA(df_p['lemmatized'], fullt, fullp, fulla, fulld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f4beb618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Topic 0 ------\n",
      "system #ichbinreyhan time work one would year working university @diballestero\n",
      "\n",
      "------ Topic 1 ------\n",
      "career contract #ichbinreyhan working condition phd one system student research\n",
      "\n",
      "------ Topic 2 ------\n",
      "#ichbinreyhan thread contract work scholar research one time teaching working\n",
      "\n",
      "------ Topic 3 ------\n",
      "thread contract system #ichbinreyhan condition #hannabeidergew permanent working one academic\n",
      "\n",
      "------ Topic 4 ------\n",
      "system contract job research work phd many researcher year career\n",
      "\n",
      "------ Topic 5 ------\n",
      "job solidarity contract work read #oneofusallofus year law conference day\n",
      "\n",
      "------ Topic 6 ------\n",
      "something year working phd postdoc even one time many permanent\n",
      "\n",
      "------ Topic 7 ------\n",
      "#ichbinreyhan want international #academicprecarity working phd great problem scientific weekend\n",
      "\n",
      "------ Topic 8 ------\n",
      "#ichbinreyhan contract research system year position postdoc job thread career\n",
      "\n",
      "\n",
      "Perplexity:  -7.817932976072848\n",
      "\n",
      "Coherence Score:  0.2668483561514083\n"
     ]
    }
   ],
   "source": [
    "eng_model_p = perform_LDA(df_en_p['lemmatized'], engt, engp, enga, engd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "36a55e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Topic 0 ------\n",
      "jahr @anjakarliczek uni #hannaimbundestag befristet forschung stellen arbeit promotion system\n",
      "\n",
      "------ Topic 1 ------\n",
      "#ichbinreyhan jahr vertrag monat uni befristet stellen #hannainzahlen stelle zeit\n",
      "\n",
      "------ Topic 2 ------\n",
      "#ichbinreyhan #tvstud arbeitsbedingung #frististfrust jahr prekär #wisssystemfehler uni arbeit hochschule\n",
      "\n",
      "------ Topic 3 ------\n",
      "#ichbinreyhan #waspostdocswoll @karolinedoering @christinaholzel @esteinhauer @klios_spiegel @tinido wissen @richterhedwig @c_kenneweg\n",
      "\n",
      "------ Topic 4 ------\n",
      "#ichbinreyhan uni jahr stellen hochschule arbeit forschung @gew_bund problem zeit\n",
      "\n",
      "------ Topic 5 ------\n",
      "#ichbinreyhan jahr @gew_bund uni forschung stellen befristet arbeit thema prekär\n",
      "\n",
      "\n",
      "Perplexity:  -8.503111455517988\n",
      "\n",
      "Coherence Score:  0.28803291130495884\n"
     ]
    }
   ],
   "source": [
    "ger_model_p = perform_LDA(df_ger_p['lemmatized'], gert, gerp, gera, gerd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9509f54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Topic 0 ------\n",
      "#ichbinreyhan #wisssystemfehler stellen problem jahr wissenschaftssystem system zeit forschung @humboldtuni\n",
      "\n",
      "------ Topic 1 ------\n",
      "#ichbinreyhan jahr professur stellen #wisssystemfehler uni befristet system befristung wissenschaftlich\n",
      "\n",
      "------ Topic 2 ------\n",
      "#wisssystemfehler #ichbinreyhan #tvstud uni hochschule arbeit jahr beschäftigt forschung lehre\n",
      "\n",
      "------ Topic 3 ------\n",
      "#ichbinreyhan jahr #wisssystemfehler arbeit uni hochschule @gew_bund studierend forschung arbeitsbedingung\n",
      "\n",
      "------ Topic 4 ------\n",
      "#ichbinreyhan @gew_bund uni perspektive schaffen beschäftigt #tvstud #dauerstell studierend jahr\n",
      "\n",
      "------ Topic 5 ------\n",
      "#ichbinreyhan #95vswisszeitvg #wisssystemfehler arbeitsbedingung hochschule zeit problem system lehre befristet\n",
      "\n",
      "------ Topic 6 ------\n",
      "problem #dauerstell zeigen berlin #wisssystemfehler system befristung #ichbinreyhan uni frage\n",
      "\n",
      "\n",
      "Perplexity:  -7.791309302311857\n",
      "\n",
      "Coherence Score:  0.25592225335715807\n"
     ]
    }
   ],
   "source": [
    "oct_model_p = perform_LDA(df_october_p['lemmatized'], octt, octp, octa, octd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "505573a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Topic 0 ------\n",
      "@gew_bund #dauerstell jahr #wissmobb uni #wisssystemfehler #aktionskonferenz #werdarfhannasein frage system\n",
      "\n",
      "------ Topic 1 ------\n",
      "zeit @jule_specht uni #dauerstell stelle befristet #wisssystemfehler problem jahr tag\n",
      "\n",
      "------ Topic 2 ------\n",
      "#wirhabenbedarf @adressel #tvstud #frististfrust #tvstudjetzt arbeitsbedingung arbeit forschung beschäftigt studentisch\n",
      "\n",
      "------ Topic 3 ------\n",
      "#tvstud uni @gew_bund #wisssystemfehler #dasgewinnenwir arbeit hochschule #frististfrust arbeitsbedingung stellen\n",
      "\n",
      "------ Topic 4 ------\n",
      "karriere lukman christopher @lisajanotta system trotz dauermobilität wissenschaftlich leisten unsicherheit\n",
      "\n",
      "------ Topic 5 ------\n",
      "#koalitionsvertrag koalitionsvertrag fdp #ampel bildung forschung @gew_bund arbeitsbedingung wichtig schaffen\n",
      "\n",
      "------ Topic 6 ------\n",
      "#wisssystemfehler hochschule #tvstud system @anja_steinbeck @gew_bund problem arbeit befristet stellen\n",
      "\n",
      "------ Topic 7 ------\n",
      "system stellen @nga_wiss @akellergew #tvstud problem zeigen woche #wisssystemfehler #koalitionsvertrag\n",
      "\n",
      "\n",
      "Perplexity:  -7.712525461258503\n",
      "\n",
      "Coherence Score:  0.34893175059633297\n"
     ]
    }
   ],
   "source": [
    "nov_model_p = perform_LDA(df_november_p['lemmatized'], novt, novp, nova, novd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "4aaafab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Topic 0 ------\n",
      "#ichbinreyhan arbeit sprechen lehre freuen #wisssystemfehler thema forschung stelle befristet\n",
      "\n",
      "------ Topic 1 ------\n",
      "#ichbinreyhan 2022 2021 stellen @humboldtuni @bverfg juni initiative erfolg einsatz\n",
      "\n",
      "------ Topic 2 ------\n",
      "#ichbinreyhan uni arbeit unbefristet befristet schön system tag forschung leute\n",
      "\n",
      "------ Topic 3 ------\n",
      "#ichbinreyhan toxisch leistungsfähigkeit stellen verein system deadlines legen uni langfristig\n",
      "\n",
      "------ Topic 4 ------\n",
      "beruflich #ichbinreyhan uni promotion jahr stellen toll lassen @realsci_de gute\n",
      "\n",
      "------ Topic 5 ------\n",
      "#ichbinreyhan @gew_bund uni @humboldtuni #wissenschaft hochschule @hrk_aktuell #dauerstell arbeit arbeitsbedingung\n",
      "\n",
      "------ Topic 6 ------\n",
      "jahr forschung #ichbinreyhan befristet vertrag stellen professur chance laufen heißen\n",
      "\n",
      "\n",
      "Perplexity:  -7.430627014442017\n",
      "\n",
      "Coherence Score:  0.313390458076842\n"
     ]
    }
   ],
   "source": [
    "dec_model_p = perform_LDA(df_december_p['lemmatized'], dect, decp, deca, decd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5642daa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Topic 0 ------\n",
      "#ichbinreyhan @w_jahr vertrag schön arbeitsbedingung #wisssystemfehler jahr wissen @starkwatzinger forschung\n",
      "\n",
      "------ Topic 1 ------\n",
      "#ichbinreyhan @mliebendoerfer @hrk_aktuell arbeit problem stellen @agehrlach @unileipzig @histodigitale job\n",
      "\n",
      "------ Topic 2 ------\n",
      "#ichbinreyhan #wisssystemfehler uni befristet frage zeit stellen thema jahr system\n",
      "\n",
      "------ Topic 3 ------\n",
      "#ichbinreyhan lehre stellen uni forschung jahr warten zeit hochschule sprechen\n",
      "\n",
      "------ Topic 4 ------\n",
      "#ichbinreyhan uni wissen problem frage lehre #wisssystemfehler jahr thema forschung\n",
      "\n",
      "------ Topic 5 ------\n",
      "#ichbinreyhan system jahr uni @jenniferhenkehb forschung frage vertrag @sainethina groß\n",
      "\n",
      "------ Topic 6 ------\n",
      "stellen schuld problem mensch @sainethina contract krank glück bedingung generation\n",
      "\n",
      "------ Topic 7 ------\n",
      "#ichbinreyhan system promotion arbeit wisszeitvg semester @mliebendoerfer schaffen @sainethina problem\n",
      "\n",
      "------ Topic 8 ------\n",
      "#ichbinreyhan problem jahr system zeit job @unileipzig @diballestero #wisssystemfehler @agehrlach\n",
      "\n",
      "\n",
      "Perplexity:  -7.800957302854855\n",
      "\n",
      "Coherence Score:  0.282501979087584\n"
     ]
    }
   ],
   "source": [
    "jan_model_p = perform_LDA(df_january_p['lemmatized'], jant, janp, jana, jand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "bb5e5a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Topic 0 ------\n",
      "#ichbinreyhan @jlugiessen @starkwatzinger #jlugiessen daueraufgabe #wissenschaft #wisssystemfehler #hannaorganisiertsich #dauerstellenfürdaueraufgaben forschung\n",
      "\n",
      "------ Topic 1 ------\n",
      "#ichbinreyhan uni system jahr #oneofusallofus @starkwatzinger #sinderechostampocohayciencia #wisssystemfehler change @gew_bund\n",
      "\n",
      "------ Topic 2 ------\n",
      "#ichbinreyhan uni zeit problem stellen jahr system groß @dievilla4 #wisssystemfehler\n",
      "\n",
      "------ Topic 3 ------\n",
      "problem @nga_wiss schlecht betrieb @_verdi uni job schön befristung lassen\n",
      "\n",
      "------ Topic 4 ------\n",
      "#ichbinreyhan jahr problem frage uni aktuell forschung #wisssystemfehler zeit kurz\n",
      "\n",
      "------ Topic 5 ------\n",
      "#ichbinreyhan thread uni karriere wissenschaftlich #wisssystemfehler stellen #oneofusallofus schön @jenniferhenkehb\n",
      "\n",
      "------ Topic 6 ------\n",
      "#ichbinreyhan #lenzen jahr forschung #wisssystemfehler thema arbeit wissen uni dieter\n",
      "\n",
      "\n",
      "Perplexity:  -7.525286288716111\n",
      "\n",
      "Coherence Score:  0.30252489562462115\n"
     ]
    }
   ],
   "source": [
    "feb_model_p = perform_LDA(df_february_p['lemmatized'], febt, febp, feba, febd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fcaa0d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Topic 0 ------\n",
      "#mentalhealth #weilwirwissenschaftlieben #promotion #hochschulpolitik #thesis_ev #ichbinreyhan postdoc jahr forschung health\n",
      "\n",
      "------ Topic 1 ------\n",
      "#ichbinreyhan uni professor jahr hochschule @jenniferhenkehb verwaltung gehalt application problem\n",
      "\n",
      "------ Topic 2 ------\n",
      "#ichbinreyhan frage jahr befristet job thema woche vertrag zeit lehre\n",
      "\n",
      "------ Topic 3 ------\n",
      "#ichbinreyhan buch @suhrkamp arbeit befristet jahr @jenniferhenkehb #wisssystemfehler tweet publikation\n",
      "\n",
      "------ Topic 4 ------\n",
      "#ichbinreyhan buch thema science arbeit jahr deutschland uni freuen system\n",
      "\n",
      "------ Topic 5 ------\n",
      "#ichbinreyhan prekär buch deutschland jahr forschung @suhrkamp uni system professur\n",
      "\n",
      "\n",
      "Perplexity:  -7.381128901208511\n",
      "\n",
      "Coherence Score:  0.3089138661857515\n"
     ]
    }
   ],
   "source": [
    "mar_model_p = perform_LDA(df_march_p['lemmatized'], mart, marp, mara, mard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ea1e8d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Topic 0 ------\n",
      "problem @maithinkx arbeitsbedingung prekär @drlutzboehm @diballestero zeit job streitschrift system\n",
      "\n",
      "------ Topic 1 ------\n",
      "@maithi_nk arbeitsbedingung @maithinkx thema #wisssystemfehler #maithinkx @gew_bund problem folge sendung\n",
      "\n",
      "------ Topic 2 ------\n",
      "@maithi_nk problem jahr uni @maithinkx kunst argument geisteswissenschaft @michael_gerloff betreffen\n",
      "\n",
      "------ Topic 3 ------\n",
      "@maithinkx arbeitsbedingung @maithi_nk problem @drlutzboehm @diballestero system prekär uni jahr\n",
      "\n",
      "\n",
      "Perplexity:  -6.8891145486229455\n",
      "\n",
      "Coherence Score:  0.28197061079472246\n"
     ]
    }
   ],
   "source": [
    "apr_model_p = perform_LDA(df_april_p['lemmatized'], aprt, aprp, apra, aprd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8de01a",
   "metadata": {},
   "source": [
    "### Pool tweets by day and user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333c6abf",
   "metadata": {},
   "source": [
    "Pool tweets by day and user to get a greater amount of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "51daae54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pu = df.groupby(['date','author.username'], as_index=False)['lemmatized'].sum()\n",
    "df_en_pu = df_en.groupby(['date','author.username'], as_index=False)['lemmatized'].sum()\n",
    "df_ger_pu = df_ger.groupby(['date','author.username'], as_index=False)['lemmatized'].sum()\n",
    "df_october_pu = df_october.groupby(['date','author.username'], as_index=False)['lemmatized'].sum()\n",
    "df_november_pu = df_november.groupby(['date','author.username'], as_index=False)['lemmatized'].sum()\n",
    "df_december_pu = df_december.groupby(['date','author.username'], as_index=False)['lemmatized'].sum()\n",
    "df_january_pu = df_january.groupby(['date','author.username'], as_index=False)['lemmatized'].sum()\n",
    "df_february_pu = df_february.groupby(['date','author.username'], as_index=False)['lemmatized'].sum()\n",
    "df_march_pu = df_march.groupby(['date','author.username'], as_index=False)['lemmatized'].sum()\n",
    "df_april_pu = df_april.groupby(['date','author.username'], as_index=False)['lemmatized'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17784cc",
   "metadata": {},
   "source": [
    "### Descriptive statistic about documents pooled by day and user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "87c02c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model:\n",
      "Number of documents:20289 Minimum word count:0 Maximum word count:716 Mean word count:15.85504460545123\n",
      "Minimum character count:0 Maximum character count:6455 Mean character count:153.2040021686628\n",
      "\n",
      "English model:\n",
      "Number of documents:2809 Minimum word count:0 Maximum word count:710 Mean word count:17.36133855464578\n",
      "Minimum character count:0 Maximum character count:6399 Mean character count:140.58027767888927\n",
      "\n",
      "German model:\n",
      "Number of documents:16987 Minimum word count:0 Maximum word count:480 Mean word count:15.704891976217107\n",
      "Minimum character count:0 Maximum character count:4819 Mean character count:155.92547242008595\n",
      "\n",
      "October model:\n",
      "Number of documents:1476 Minimum word count:0 Maximum word count:194 Mean word count:15.563685636856368\n",
      "Minimum character count:0 Maximum character count:1885 Mean character count:154.94850948509486\n",
      "\n",
      "November model:\n",
      "Number of documents:1366 Minimum word count:0 Maximum word count:282 Mean word count:15.759882869692532\n",
      "Minimum character count:0 Maximum character count:2789 Mean character count:156.98389458272328\n",
      "\n",
      "December model:\n",
      "Number of documents:779 Minimum word count:0 Maximum word count:162 Mean word count:14.048780487804878\n",
      "Minimum character count:0 Maximum character count:1519 Mean character count:137.65468549422337\n",
      "\n",
      "January model:\n",
      "Number of documents:971 Minimum word count:0 Maximum word count:122 Mean word count:14.367662203913492\n",
      "Minimum character count:0 Maximum character count:1361 Mean character count:139.64263645726055\n",
      "\n",
      "February model:\n",
      "Number of documents:804 Minimum word count:0 Maximum word count:30 Mean word count:15.29726368159204\n",
      "Minimum character count:0 Maximum character count:1267 Mean character count:149.2226368159204\n",
      "\n",
      "March model:\n",
      "Number of documents:746 Minimum word count:0 Maximum word count:35 Mean word count:13.735924932975871\n",
      "Minimum character count:0 Maximum character count:1729 Mean character count:131.18498659517425\n",
      "\n",
      "April model:\n",
      "Number of documents:522 Minimum word count:0 Maximum word count:28 Mean word count:13.270114942528735\n",
      "Minimum character count:0 Maximum character count:1914 Mean character count:129.32950191570882\n"
     ]
    }
   ],
   "source": [
    "print(\"Full model:\")\n",
    "print(\"Number of documents:\" + str(len(df_pu)) +\" Minimum word count:\" + str(min(df_pu['lemmatized'].map(len))) + \" Maximum word count:\" + str(max(df_pu['lemmatized'].map(len))) + \" Mean word count:\" + str(df_pu['lemmatized'].map(len).mean()))\n",
    "print(\"Minimum character count:\" + str(min(df_pu['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Maximum character count:\" + str(max(df_pu['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Mean character count:\" +str(df_pu['lemmatized'].apply(lambda x: ' '.join(x)).map(len).mean()))\n",
    "\n",
    "print(\"\\nEnglish model:\")\n",
    "print(\"Number of documents:\" + str(len(df_en_pu)) +\" Minimum word count:\" + str(min(df_en_pu['lemmatized'].map(len))) + \" Maximum word count:\" + str(max(df_en_pu['lemmatized'].map(len))) + \" Mean word count:\" + str(df_en_pu['lemmatized'].map(len).mean()))\n",
    "print(\"Minimum character count:\" + str(min(df_en_pu['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Maximum character count:\" + str(max(df_en_pu['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Mean character count:\" +str(df_en_pu['lemmatized'].apply(lambda x: ' '.join(x)).map(len).mean()))\n",
    "\n",
    "print(\"\\nGerman model:\")\n",
    "print(\"Number of documents:\" + str(len(df_ger_pu)) +\" Minimum word count:\" + str(min(df_ger_pu['lemmatized'].map(len))) + \" Maximum word count:\" + str(max(df_ger_pu['lemmatized'].map(len))) + \" Mean word count:\" + str(df_ger_pu['lemmatized'].map(len).mean()))\n",
    "print(\"Minimum character count:\" + str(min(df_ger_pu['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Maximum character count:\" + str(max(df_ger_pu['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Mean character count:\" +str(df_ger_pu['lemmatized'].apply(lambda x: ' '.join(x)).map(len).mean()))\n",
    "\n",
    "print(\"\\nOctober model:\")\n",
    "print(\"Number of documents:\" + str(len(df_october_pu)) +\" Minimum word count:\" + str(min(df_october_pu['lemmatized'].map(len))) + \" Maximum word count:\" + str(max(df_october_pu['lemmatized'].map(len))) + \" Mean word count:\" + str(df_october_pu['lemmatized'].map(len).mean()))\n",
    "print(\"Minimum character count:\" + str(min(df_october_pu['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Maximum character count:\" + str(max(df_october_pu['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Mean character count:\" +str(df_october_pu['lemmatized'].apply(lambda x: ' '.join(x)).map(len).mean()))\n",
    "\n",
    "print(\"\\nNovember model:\")\n",
    "print(\"Number of documents:\" + str(len(df_november_pu)) +\" Minimum word count:\" + str(min(df_november_pu['lemmatized'].map(len))) + \" Maximum word count:\" + str(max(df_november_pu['lemmatized'].map(len))) + \" Mean word count:\" + str(df_november_pu['lemmatized'].map(len).mean()))\n",
    "print(\"Minimum character count:\" + str(min(df_november_pu['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Maximum character count:\" + str(max(df_november_pu['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Mean character count:\" +str(df_november_pu['lemmatized'].apply(lambda x: ' '.join(x)).map(len).mean()))\n",
    "\n",
    "print(\"\\nDecember model:\")\n",
    "print(\"Number of documents:\" + str(len(df_december_pu)) +\" Minimum word count:\" + str(min(df_december_pu['lemmatized'].map(len))) + \" Maximum word count:\" + str(max(df_december_pu['lemmatized'].map(len))) + \" Mean word count:\" + str(df_december_pu['lemmatized'].map(len).mean()))\n",
    "print(\"Minimum character count:\" + str(min(df_december_pu['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Maximum character count:\" + str(max(df_december_pu['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Mean character count:\" +str(df_december_pu['lemmatized'].apply(lambda x: ' '.join(x)).map(len).mean()))\n",
    "\n",
    "print(\"\\nJanuary model:\")\n",
    "print(\"Number of documents:\" + str(len(df_january_pu)) +\" Minimum word count:\" + str(min(df_january_pu['lemmatized'].map(len))) + \" Maximum word count:\" + str(max(df_january_pu['lemmatized'].map(len))) + \" Mean word count:\" + str(df_january_pu['lemmatized'].map(len).mean()))\n",
    "print(\"Minimum character count:\" + str(min(df_january_pu['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Maximum character count:\" + str(max(df_january_pu['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Mean character count:\" +str(df_january_pu['lemmatized'].apply(lambda x: ' '.join(x)).map(len).mean()))\n",
    "\n",
    "print(\"\\nFebruary model:\")\n",
    "print(\"Number of documents:\" + str(len(df_february_pu)) +\" Minimum word count:\" + str(min(df_february_pu['lemmatized'].map(len))) + \" Maximum word count:\" + str(max(df_february['lemmatized'].map(len))) + \" Mean word count:\" + str(df_february_pu['lemmatized'].map(len).mean()))\n",
    "print(\"Minimum character count:\" + str(min(df_february_pu['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Maximum character count:\" + str(max(df_february_pu['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Mean character count:\" +str(df_february_pu['lemmatized'].apply(lambda x: ' '.join(x)).map(len).mean()))\n",
    "\n",
    "print(\"\\nMarch model:\")\n",
    "print(\"Number of documents:\" + str(len(df_march_pu)) +\" Minimum word count:\" + str(min(df_march_pu['lemmatized'].map(len))) + \" Maximum word count:\" + str(max(df_march['lemmatized'].map(len))) + \" Mean word count:\" + str(df_march_pu['lemmatized'].map(len).mean()))\n",
    "print(\"Minimum character count:\" + str(min(df_march_pu['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Maximum character count:\" + str(max(df_march_pu['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Mean character count:\" +str(df_march_pu['lemmatized'].apply(lambda x: ' '.join(x)).map(len).mean()))\n",
    "\n",
    "print(\"\\nApril model:\")\n",
    "print(\"Number of documents:\" + str(len(df_april_pu)) +\" Minimum word count:\" + str(min(df_april_pu['lemmatized'].map(len))) + \" Maximum word count:\" + str(max(df_april['lemmatized'].map(len))) + \" Mean word count:\" + str(df_april_pu['lemmatized'].map(len).mean()))\n",
    "print(\"Minimum character count:\" + str(min(df_april_pu['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Maximum character count:\" + str(max(df_april_pu['lemmatized'].apply(lambda x: ' '.join(x)).map(len))) + \" Mean character count:\" +str(df_april_pu['lemmatized'].apply(lambda x: ' '.join(x)).map(len).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "cc58a7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4, 5, 6, 7, 8, 9] [0.3398847006943793, 0.34529539840731954, 0.3923369930674224, 0.3957666356280699, 0.4041385621712125, 0.3974320370078908, 0.4318614592145518, 0.42701985072551096]\n",
      "[5, 10, 15, 20] [0.4243476061331424, 0.4311417156296843, 0.42864149264307316, 0.43026328855690865]\n",
      "['symmetric', 'asymmetric'] [0.4311417156296843, 0.4042443234256079]\n",
      "[0.5, 0.7, 0.9] [0.4311417156296843, 0.4298871586743064, 0.4243476061331423]\n"
     ]
    }
   ],
   "source": [
    "dect_pu = compute_coherence_values_topics(df_december_pu['lemmatized'])\n",
    "decp_pu = compute_coherence_values_passes(df_december_pu['lemmatized'], dect_pu)\n",
    "deca_pu = compute_coherence_values_alpha(df_december_pu['lemmatized'], dect_pu, decp_pu)\n",
    "decd_pu = compute_coherence_values_decay(df_december_pu['lemmatized'], dect_pu, decp_pu, deca_pu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ecc10d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4, 5, 6, 7, 8, 9] [0.4266355474197704, 0.4506344321145607, 0.44813446084570474, 0.4409750951660386, 0.4506052427970581, 0.44987094999799676, 0.4658268694118159, 0.48502728014214735]\n",
      "[2, 3, 4, 5, 6, 7, 8, 9] [0.3161630734372046, 0.31257998946885185, 0.3216386458905185, 0.3185813908284758, 0.31758053545511566, 0.3242787668691878, 0.31929463423126847, 0.32485794287718694]\n",
      "[2, 3, 4, 5, 6, 7, 8, 9] [0.38927406728988956, 0.42725807049291253, 0.42009845222859066, 0.44423351695564034, 0.45041192468663166, 0.4744489534083417, 0.46791299161086586, 0.4618251342821497]\n",
      "[2, 3, 4, 5, 6, 7, 8, 9] [0.215059282883793, 0.19949391608984354, 0.2341504110109347, 0.2518185857876336, 0.2848952096323821, 0.29400656303802924, 0.30124721752237815, 0.2923999705756573]\n",
      "[2, 3, 4, 5, 6, 7, 8, 9] [0.19032827879353026, 0.20383356144268983, 0.2054786070237673, 0.22630459588950388, 0.2351773985212662, 0.2543386263853612, 0.2606413340628617, 0.2764395180519623]\n",
      "[2, 3, 4, 5, 6, 7, 8, 9] [0.3675324445974824, 0.3844568147349598, 0.39834571465988083, 0.38072013598612436, 0.41112996168146343, 0.43562847443439134, 0.42456879979455486, 0.4553729450631507]\n",
      "[2, 3, 4, 5, 6, 7, 8, 9] [0.3191881888048145, 0.29505644619900134, 0.3372958357564063, 0.364119561136971, 0.3838314984232727, 0.38490108707945847, 0.3888845684128519, 0.4011469101942111]\n",
      "[2, 3, 4, 5, 6, 7, 8, 9] [0.3675324445974824, 0.3844568147349598, 0.39834571465988083, 0.38072013598612436, 0.41112996168146343, 0.43562847443439134, 0.42456879979455486, 0.4553729450631507]\n",
      "[2, 3, 4, 5, 6, 7, 8, 9] [0.36936106223537624, 0.42424758548220315, 0.43399215102110866, 0.40574984026553473, 0.45149773510052676, 0.4678671316703606, 0.4621986082012032, 0.45766902748213717]\n",
      "[2, 3, 4, 5, 6, 7, 8, 9] [0.44312200682843284, 0.5031151029856731, 0.4990994891747418, 0.4878865029965538, 0.4718522562870075, 0.47009521500460444, 0.5152820981375139, 0.4923629366854916]\n",
      "[5, 10, 15, 20] [0.4447577527636365, 0.5472809827264338, 0.5583763867770926, 0.5431268555547464]\n",
      "[5, 10, 15, 20] [0.32271051024239694, 0.3440574809766395, 0.3417524376305907, 0.3546870947268498]\n",
      "[5, 10, 15, 20] [0.48376233342844605, 0.4814434561505478, 0.48182787230867546, 0.48568108044751296]\n",
      "[5, 10, 15, 20] [0.28017765306535647, 0.27920153539940973, 0.2826148597145831, 0.2906218880796797]\n",
      "[5, 10, 15, 20] [0.3712993614983453, 0.39782789015611164, 0.39742887447372505, 0.37600293539350726]\n",
      "[5, 10, 15, 20] [0.4327937206194347, 0.4262497860232155, 0.4293794996429237, 0.4307709687829024]\n",
      "[5, 10, 15, 20] [0.40110821830813936, 0.41611975638216075, 0.41759480907211255, 0.4191957033479546]\n",
      "[5, 10, 15, 20] [0.4327937206194347, 0.4262497860232155, 0.4293794996429237, 0.4307709687829024]\n",
      "[5, 10, 15, 20] [0.4431445792287104, 0.4394342410579205, 0.4426210954978726, 0.4471537027170592]\n",
      "[5, 10, 15, 20] [0.48319890034816615, 0.49499660404616597, 0.48204384369081504, 0.47938618288024537]\n",
      "['symmetric', 'asymmetric'] [0.5566063783043484, 0.5135778278409245]\n",
      "['symmetric', 'asymmetric'] [0.3546870947268498, 0.34021118797572203]\n",
      "['symmetric', 'asymmetric'] [0.49156852577786, 0.4450511551518413]\n",
      "['symmetric', 'asymmetric'] [0.2906218880796797, 0.32660130707697366]\n",
      "['symmetric', 'asymmetric'] [0.39782789015611164, 0.35528176522717153]\n",
      "['symmetric', 'asymmetric'] [0.4327937206194347, 0.4471446040682989]\n",
      "['symmetric', 'asymmetric'] [0.4191957033479546, 0.40284326540092796]\n",
      "['symmetric', 'asymmetric'] [0.4327937206194347, 0.4471446040682989]\n",
      "['symmetric', 'asymmetric'] [0.4471537027170592, 0.4288715159021191]\n",
      "['symmetric', 'asymmetric'] [0.4922300970015827, 0.505723819425413]\n",
      "[0.5, 0.7, 0.9] [0.5583763867770926, 0.5257178775924715, 0.44985596622544477]\n",
      "[0.5, 0.7, 0.9] [0.3546870947268498, 0.342608078523017, 0.32648963038393164]\n",
      "[0.5, 0.7, 0.9] [0.4902339428144477, 0.4842110105219285, 0.4879203921092454]\n",
      "[0.5, 0.7, 0.9] [0.32660130707697366, 0.3329108721672872, 0.32687888109242036]\n",
      "[0.5, 0.7, 0.9] [0.39782789015611164, 0.374578106409789, 0.3738690514150953]\n",
      "[0.5, 0.7, 0.9] [0.4471446040682989, 0.438490539573902, 0.43721229306704285]\n",
      "[0.5, 0.7, 0.9] [0.4191957033479546, 0.4177051773826882, 0.4100455264641752]\n",
      "[0.5, 0.7, 0.9] [0.4471446040682989, 0.438490539573902, 0.43721229306704285]\n",
      "[0.5, 0.7, 0.9] [0.4471537027170592, 0.4409186922711465, 0.4415119834916586]\n",
      "[0.5, 0.7, 0.9] [0.505723819425413, 0.4993077786737171, 0.49857355202661474]\n"
     ]
    }
   ],
   "source": [
    "#full data\n",
    "fullt_pu = compute_coherence_values_topics(df_pu['lemmatized'])\n",
    "#english data\n",
    "engt_pu = compute_coherence_values_topics(df_en_pu['lemmatized'])\n",
    "#german data\n",
    "gert_pu = compute_coherence_values_topics(df_ger_pu['lemmatized'])\n",
    "#october data\n",
    "octt_pu = compute_coherence_values_topics(df_october_pu['lemmatized'])\n",
    "#november data\n",
    "novt_pu = compute_coherence_values_topics(df_november_pu['lemmatized'])\n",
    "#december data\n",
    "dect_pu = compute_coherence_values_topics(df_december_pu['lemmatized'])\n",
    "#january data\n",
    "jant_pu = compute_coherence_values_topics(df_january_pu['lemmatized'])\n",
    "#february data\n",
    "febt_pu = compute_coherence_values_topics(df_february_pu['lemmatized'])\n",
    "#march data\n",
    "mart_pu = compute_coherence_values_topics(df_march_pu['lemmatized'])\n",
    "#april data\n",
    "aprt_pu = compute_coherence_values_topics(df_april_pu['lemmatized'])\n",
    "\n",
    "#full data\n",
    "fullp_pu = compute_coherence_values_passes(df_pu['lemmatized'], fullt_pu)\n",
    "#english data\n",
    "engp_pu = compute_coherence_values_passes(df_en_pu['lemmatized'], engt_pu)\n",
    "#german data\n",
    "gerp_pu = compute_coherence_values_passes(df_ger_pu['lemmatized'], gert_pu)\n",
    "#october data\n",
    "octp_pu = compute_coherence_values_passes(df_october_pu['lemmatized'], octt_pu)\n",
    "#november data\n",
    "novp_pu = compute_coherence_values_passes(df_november_pu['lemmatized'], novt_pu)\n",
    "#december data\n",
    "decp_pu = compute_coherence_values_passes(df_december_pu['lemmatized'], dect_pu)\n",
    "#january data\n",
    "janp_pu = compute_coherence_values_passes(df_january_pu['lemmatized'], jant_pu)\n",
    "#february data\n",
    "febp_pu = compute_coherence_values_passes(df_february_pu['lemmatized'], febt_pu)\n",
    "#march data\n",
    "marp_pu = compute_coherence_values_passes(df_march_pu['lemmatized'], mart_pu)\n",
    "#april data\n",
    "aprp_pu = compute_coherence_values_passes(df_april_pu['lemmatized'], aprt_pu)\n",
    "\n",
    "#full data\n",
    "fulla_pu = compute_coherence_values_alpha(df_pu['lemmatized'], fullt_pu, fullp_pu)\n",
    "#english data\n",
    "enga_pu = compute_coherence_values_alpha(df_en_pu['lemmatized'], engt_pu, engp_pu)\n",
    "#german data\n",
    "gera_pu = compute_coherence_values_alpha(df_ger_pu['lemmatized'], gert_pu, gerp_pu)\n",
    "#october data\n",
    "octa_pu = compute_coherence_values_alpha(df_october_pu['lemmatized'], octt_pu, octp_pu)\n",
    "#november data\n",
    "nova_pu = compute_coherence_values_alpha(df_november_pu['lemmatized'], novt_pu, novp_pu)\n",
    "#december data\n",
    "deca_pu = compute_coherence_values_alpha(df_december_pu['lemmatized'], dect_pu, decp_pu)\n",
    "#january data\n",
    "jana_pu = compute_coherence_values_alpha(df_january_pu['lemmatized'], jant_pu, janp_pu)\n",
    "#february data\n",
    "feba_pu = compute_coherence_values_alpha(df_february_pu['lemmatized'], febt_pu, febp_pu)\n",
    "#march data\n",
    "mara_pu = compute_coherence_values_alpha(df_march_pu['lemmatized'], mart_pu, marp_pu)\n",
    "#april data\n",
    "apra_pu = compute_coherence_values_alpha(df_april_pu['lemmatized'], aprt_pu, aprp_pu)\n",
    "\n",
    "#full data\n",
    "fulld_pu = compute_coherence_values_decay(df_pu['lemmatized'], fullt_pu, fullp_pu, fulla_pu)\n",
    "#english data\n",
    "engd_pu = compute_coherence_values_decay(df_en_pu['lemmatized'], engt_pu, engp_pu, enga_pu)\n",
    "#german data\n",
    "gerd_pu = compute_coherence_values_decay(df_ger_pu['lemmatized'], gert_pu, gerp_pu, gera_pu)\n",
    "#october data\n",
    "octd_pu = compute_coherence_values_decay(df_october_pu['lemmatized'], octt_pu, octp_pu, octa_pu)\n",
    "#november data\n",
    "novd_pu = compute_coherence_values_decay(df_november_pu['lemmatized'], novt_pu, novp_pu, nova_pu)\n",
    "#december data\n",
    "decd_pu = compute_coherence_values_decay(df_december_pu['lemmatized'], dect_pu, decp_pu, deca_pu)\n",
    "#january data\n",
    "jand_pu = compute_coherence_values_decay(df_january_pu['lemmatized'], jant_pu, janp_pu, jana_pu)\n",
    "#february data\n",
    "febd_pu = compute_coherence_values_decay(df_february_pu['lemmatized'], febt_pu, febp_pu, feba_pu)\n",
    "#march data\n",
    "mard_pu = compute_coherence_values_decay(df_march_pu['lemmatized'], mart_pu, marp_pu, mara_pu)\n",
    "#april data\n",
    "aprd_pu = compute_coherence_values_decay(df_april_pu['lemmatized'], aprt_pu, aprp_pu, apra_pu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "119e93d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 15 symmetric 0.5\n",
      "9 20 symmetric 0.5\n",
      "7 20 symmetric 0.5\n",
      "8 20 asymmetric 0.7\n",
      "9 10 symmetric 0.5\n",
      "8 10 symmetric 0.5\n",
      "9 20 symmetric 0.5\n",
      "9 5 asymmetric 0.5\n",
      "7 20 symmetric 0.5\n",
      "8 10 asymmetric 0.5\n"
     ]
    }
   ],
   "source": [
    "print(fullt_pu,fullp_pu,fulla_pu,fulld_pu)\n",
    "print(engt_pu,engp_pu,enga_pu,engd_pu)\n",
    "print(gert_pu,gerp_pu,gera_pu,gerd_pu)\n",
    "print(octt_pu,octp_pu,octa_pu,octd_pu)\n",
    "print(novt_pu,novp_pu,nova_pu,novd_pu)\n",
    "print(dect_pu,decp_pu,deca_pu,decd_pu)\n",
    "print(jant_pu,janp_pu,jana_pu,jand_pu)\n",
    "print(febt_pu,febp_pu,feba_pu,febd_pu)\n",
    "print(mart_pu,marp_pu,mara_pu,mard_pu)\n",
    "print(aprt_pu,aprp_pu,apra_pu,aprd_pu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b6b56c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Topic 0 ------\n",
      "#hannaimbundestag uni wissen jahr befristet frage #ichbinreyhan stellen denken thema\n",
      "\n",
      "------ Topic 1 ------\n",
      "vertrag jahr befristet uni arbeit hochschule stellen zeit monat deutschland\n",
      "\n",
      "------ Topic 2 ------\n",
      "#ichbinreyhan uni #tvstud hochschule wissenschaftlich beschäftigt sprechen #wisssystemfehler uhr arbeitsbedingung\n",
      "\n",
      "------ Topic 3 ------\n",
      "@gew_bund #dauerstell jahr befristet stelle arbeit stellen system monat unbefristet\n",
      "\n",
      "------ Topic 4 ------\n",
      "#ichbinreyhan contract system job phd research one work year career\n",
      "\n",
      "------ Topic 5 ------\n",
      "#wissenschaft thread #ichbinreyhan system studieren #thesis_ev @gew_bund problem hochschule wichtig\n",
      "\n",
      "------ Topic 6 ------\n",
      "jahr stelle promotion stellen befristet uni geld job fest zeit\n",
      "\n",
      "------ Topic 7 ------\n",
      "@anjakarliczek jahr forschung problem uni prekär #hannaimbundestag befristet mittelbau leute\n",
      "\n",
      "------ Topic 8 ------\n",
      "forschung lehre stellen arbeitsbedingung arbeit aktuell debatte problem deutsch #ichbinreyhan\n",
      "\n",
      "\n",
      "Perplexity:  -8.84623271601472\n",
      "\n",
      "Coherence Score:  0.5321621626102322\n"
     ]
    }
   ],
   "source": [
    "full_model_pu = perform_LDA(df_pu['lemmatized'], fullt_pu, fullp_pu, fulla_pu, fulld_pu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9e09bcaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Topic 0 ------\n",
      "research system work phd career get position contract need scientist\n",
      "\n",
      "------ Topic 1 ------\n",
      "thread job one many academic time #ichbinreyhan university permanent work\n",
      "\n",
      "------ Topic 2 ------\n",
      "story great share english join thanks check system want summary\n",
      "\n",
      "------ Topic 3 ------\n",
      "want research system #ichbinreyhan job researcher need get career know\n",
      "\n",
      "------ Topic 4 ------\n",
      "postdoc phd contract permanent job condition working #hannaimbundestag discussion fight\n",
      "\n",
      "------ Topic 5 ------\n",
      "contract year system phd one university position many law researcher\n",
      "\n",
      "------ Topic 6 ------\n",
      "#ichbinreyhan work student problem much like system research scholar teaching\n",
      "\n",
      "------ Topic 7 ------\n",
      "scholar @mahaelhissy without passport people white movement @diballestero like need\n",
      "\n",
      "------ Topic 8 ------\n",
      "problem condition working precarious #ichbinreyhan researcher colleague well solidarity position\n",
      "\n",
      "\n",
      "Perplexity:  -7.90127733766446\n",
      "\n",
      "Coherence Score:  0.3276307286866858\n"
     ]
    }
   ],
   "source": [
    "eng_model_pu = perform_LDA(df_en_pu['lemmatized'], engt_pu, engp_pu, enga_pu, engd_pu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7abe7268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Topic 0 ------\n",
      "jahr uni frage system kind leute zeit stellen stelle forschung\n",
      "\n",
      "------ Topic 1 ------\n",
      "jahr uni befristet stelle stellen studieren unbefristet vertrag job wissen\n",
      "\n",
      "------ Topic 2 ------\n",
      "#ichbinreyhan system stellen problem groß befristung arbeit zeigen hochschule deutschland\n",
      "\n",
      "------ Topic 3 ------\n",
      "problem @anjakarliczek stellen forschung zeit arbeit hochschule system aktuell #wissenschaft\n",
      "\n",
      "------ Topic 4 ------\n",
      "jahr befristet forschung arbeit wissen @anjakarliczek lehre wissenschaftlich prekär arbeitsbedingung\n",
      "\n",
      "------ Topic 5 ------\n",
      "@gew_bund #ichbinreyhan #dauerstell #frististfrust forschung vertrag #tvstud daueraufgabe thread @akellergew\n",
      "\n",
      "------ Topic 6 ------\n",
      "#hannaimbundestag uni @anjakarliczek jahr hochschule prekär arbeitsbedingung beschäftigt arbeit #ichbinreyhan\n",
      "\n",
      "\n",
      "Perplexity:  -8.712104676928105\n",
      "\n",
      "Coherence Score:  0.47703880676723903\n"
     ]
    }
   ],
   "source": [
    "ger_model_pu = perform_LDA(df_ger_pu['lemmatized'], gert_pu, gerp_pu, gera_pu, gerd_pu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e509c42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Topic 0 ------\n",
      "#wisssystemfehler #ichbinreyhan jahr hochschule uni system forschung frage wissenschaftlich arbeitsbedingung\n",
      "\n",
      "------ Topic 1 ------\n",
      "deutsch contract #wisssystemfehler wissenschaftssystem monat antrag uni schaffen stelle beitrag\n",
      "\n",
      "------ Topic 2 ------\n",
      "#wisssystemfehler zeit jahr uni stelle forschung debatte studierend entfristen berliner\n",
      "\n",
      "------ Topic 3 ------\n",
      "#ichbinreyhan thread #wisssystemfehler projekt buch arbeitsbedingung #dauerstell person @humboldtuni schön\n",
      "\n",
      "------ Topic 4 ------\n",
      "#tvstud @gew_bund #dasgewinnenwir beschäftigt #frististfrust uhr #unverzichtbar studentisch #hannastreikt #keineausnahme\n",
      "\n",
      "------ Topic 5 ------\n",
      "#tvstud uni #stopthecuts #streiksemester arbeit hochschule studierend @hrk_aktuell politisch besetzen\n",
      "\n",
      "------ Topic 6 ------\n",
      "#vhdresolution @thstockinger @emmaquardt @vhdtweets @mpoessel problem @docbio1509 @jancloppenburg jahr @tobias_schulze\n",
      "\n",
      "------ Topic 7 ------\n",
      "#ichbinreyhan jahr nachwuchs #wisssystemfehler forschung stelle stellen richtig system akademisch\n",
      "\n",
      "\n",
      "Perplexity:  -7.984423620346692\n",
      "\n",
      "Coherence Score:  0.35237471076936794\n"
     ]
    }
   ],
   "source": [
    "oct_model_pu = perform_LDA(df_october_pu['lemmatized'], octt_pu, octp_pu, octa_pu, octd_pu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f7587083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Topic 0 ------\n",
      "#tvstud #wirhabenbedarf #frististfrust #dasgewinnenwir beschäftigt @gew_bund @adressel hochschule arbeitsbedingung #tvstudjetzt\n",
      "\n",
      "------ Topic 1 ------\n",
      "#ichbinreyhan vertrag uni befristet @anja_steinbeck #wisssystemfehler interessant schaffen entscheidung chance\n",
      "\n",
      "------ Topic 2 ------\n",
      "@gew_bund #dauerstell daueraufgabe #ichbinreyhan #aktionskonferenz frage #berlhg @faznet zeit debatte\n",
      "\n",
      "------ Topic 3 ------\n",
      "#ichbinreyhan #wisssystemfehler uni leute system koalitionsvertrag berliner möglichkeit verbesserung bezahlen\n",
      "\n",
      "------ Topic 4 ------\n",
      "#ichbinreyhan problem #wisssystemfehler wichtig berlin thema forschung stellen befristet @gew_bund\n",
      "\n",
      "------ Topic 5 ------\n",
      "#wissmobb #werdarfhannasein trend postdoc uni geld stellen ändern universität zahlen\n",
      "\n",
      "------ Topic 6 ------\n",
      "system forschung uni jahr hochschule karriere deutschland wissenschaftlich zeit #wisssystemfehler\n",
      "\n",
      "------ Topic 7 ------\n",
      "#ichbinreyhan @nga_wiss @akellergew jahr #koalitionsvertrag @jensjot @grundmar uhr @gew_bund arbeitsbedingung\n",
      "\n",
      "------ Topic 8 ------\n",
      "#ichbinreyhan system #wisssystemfehler studium professur via arbeit frage #wissenschaft akademisch\n",
      "\n",
      "\n",
      "Perplexity:  -7.938316132257876\n",
      "\n",
      "Coherence Score:  0.35939464175779323\n"
     ]
    }
   ],
   "source": [
    "nov_model_pu = perform_LDA(df_november_pu['lemmatized'], novt_pu, novp_pu, nova_pu, novd_pu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "fe2b23cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Topic 0 ------\n",
      "@gew_bund uni stelle weihnachten stellen #ichbinreyhan arbeit unbefristet frage leute\n",
      "\n",
      "------ Topic 1 ------\n",
      "#ichbinreyhan #dauerstell universität hochschule uni situation @starkwatzinger @gew_bund beschäftigt jahr\n",
      "\n",
      "------ Topic 2 ------\n",
      "#ichbinreyhan @hrk_aktuell #hrkadvent türchen #adventskalender lehre liebe uni system urlaub\n",
      "\n",
      "------ Topic 3 ------\n",
      "#berlhg richtig prekär @gew_bund gerne jahr zeit problem unterfinanzierung freuen\n",
      "\n",
      "------ Topic 4 ------\n",
      "#wissenschaft @humboldtuni #ichbinreyhan @gew_bund lassen @bverfg #oneofusallofus solidarity university glückwunsch\n",
      "\n",
      "------ Topic 5 ------\n",
      "#ichbinreyhan prekär forschung befristet arbeit #ichbinhannaat #frististfrust sprechen stellen #ugnovelle\n",
      "\n",
      "------ Topic 6 ------\n",
      "#ichbinreyhan jahr zeit #wisssystemfehler befristet forschung uhr hochschule gute forderung\n",
      "\n",
      "------ Topic 7 ------\n",
      "#ichbinreyhan 2021 uni promotion folge lehre schaffen double-binds stellen arbeitsbedingung\n",
      "\n",
      "\n",
      "Perplexity:  -7.723109040144352\n",
      "\n",
      "Coherence Score:  0.4253304195400586\n"
     ]
    }
   ],
   "source": [
    "dec_model_pu = perform_LDA(df_december_pu['lemmatized'], dect_pu, decp_pu, deca_pu, decd_pu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9dd939ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Topic 0 ------\n",
      "@diballestero system 2021 problem white job jahr vertrag deutsch @jenniferhenkehb\n",
      "\n",
      "------ Topic 1 ------\n",
      "frage perspektive arbeitsbedingung thema sprechen @hrk_aktuell @karolinedoering universität @gew_bund system\n",
      "\n",
      "------ Topic 2 ------\n",
      "#ichbinreyhan #wisssystemfehler system zeit thread forschung problem #thesis_ev stellen uni\n",
      "\n",
      "------ Topic 3 ------\n",
      "wissen arbeit zeit @starkwatzinger stellen forschung jahr hochschule professur pandemie\n",
      "\n",
      "------ Topic 4 ------\n",
      "#ichbinreyhan jahr frage #wisssystemfehler stellen teil #frististfrust woche promotion job\n",
      "\n",
      "------ Topic 5 ------\n",
      "@unileipzig @agehrlach @histodigitale @mliebendoerfer @unv_nunftbegabt @piczenik1 stellen liebe woche befristet\n",
      "\n",
      "------ Topic 6 ------\n",
      "#ichbinreyhan thema #wisssystemfehler #thesis_ev professur system studierend stelle #wissenschaft #dauerstell\n",
      "\n",
      "------ Topic 7 ------\n",
      "uni lehre job problem forschung befristet jahr mittelbau denken @jenniferhenkehb\n",
      "\n",
      "------ Topic 8 ------\n",
      "uni @hrk_aktuell reden prekär bild sprechen vertrag netzwerk lieber akademisch\n",
      "\n",
      "\n",
      "Perplexity:  -7.942666643515808\n",
      "\n",
      "Coherence Score:  0.4155960048562518\n"
     ]
    }
   ],
   "source": [
    "jan_model_pu = perform_LDA(df_january_pu['lemmatized'], jant_pu, janp_pu, jana_pu, jand_pu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ff74c983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Topic 0 ------\n",
      "uni #ichbinreyhan zeit forschung jahr woche #wisssystemfehler @dievilla4 thema lassen\n",
      "\n",
      "------ Topic 1 ------\n",
      "#ichbinreyhan wissen #academicprecarity unbefristet thread international #wisssystemfehler lehre forschung jahr\n",
      "\n",
      "------ Topic 2 ------\n",
      "#ichbinreyhan jahr #wissenschaft thema #phdlife prekär gewerkschaft finanziell #firstgen #thesis_ev\n",
      "\n",
      "------ Topic 3 ------\n",
      "#ichbinreyhan problem @starkwatzinger @jenniferhenkehb kreativität frist druck sicherheit fest @rudolfkipp\n",
      "\n",
      "------ Topic 4 ------\n",
      "#ichbinreyhan #hannaorganisiertsich #oneofusallofus #fb03 #studiereningiessen #ichbinspielbein #jlugiessen #giessen #unigöttingen #dauerstellenfürdaueraufgaben\n",
      "\n",
      "------ Topic 5 ------\n",
      "#lenzen dieter @jlugiessen #jlugiessen #ichbinreyhan #wisssystemfehler uni zukunft kundgebung studierend\n",
      "\n",
      "------ Topic 6 ------\n",
      "@gew_bund wissen @starkwatzinger @sebblki #ichbinreyhan daueraufgabe ziel #dauerstell mitarb 2/2\n",
      "\n",
      "------ Topic 7 ------\n",
      "saliva leben befristung marginalisiert schaffen ignorieren @_verdi schlecht pay land\n",
      "\n",
      "------ Topic 8 ------\n",
      "stellen echt minen system frage erfahrung #wissenschaft wissenschaftlich #lenzen leute\n",
      "\n",
      "\n",
      "Perplexity:  -7.7528596319387875\n",
      "\n",
      "Coherence Score:  0.42561256665803027\n"
     ]
    }
   ],
   "source": [
    "feb_model_pu = perform_LDA(df_february_pu['lemmatized'], febt_pu, febp_pu, feba_pu, febd_pu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c8d5a293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Topic 0 ------\n",
      "frage professur jahr befristet professor application position uni #mint vortrag\n",
      "\n",
      "------ Topic 1 ------\n",
      "#weilwirwissenschaftlieben #mentalhealth system #thesis_ev #ichbinreyhan #promotion #hochschulpolitik freuen forschung #ilovescience\n",
      "\n",
      "------ Topic 2 ------\n",
      "postdoc @jenniferhenkehb stelle system job diskussion buch jahr #eu reform\n",
      "\n",
      "------ Topic 3 ------\n",
      "forschung @gew_bund problem befristet lehre kind #ichbinreyhan frage freuen jahr\n",
      "\n",
      "------ Topic 4 ------\n",
      "deutschland arbeit @jenniferhenkehb wissenschaftlich @starkwatzinger #berlhg #wissenschaft buch problem woche\n",
      "\n",
      "------ Topic 5 ------\n",
      "prekär uni deutschland #ichbinreyhan jahr idiotisch sachbuch wissenschaftssystem befristet arbeitsbedingung\n",
      "\n",
      "------ Topic 6 ------\n",
      "#ichbinreyhan buch @suhrkamp jahr tag #wisssystemfehler vertrag hochschule arbeit forschung\n",
      "\n",
      "\n",
      "Perplexity:  -7.6016072439228015\n",
      "\n",
      "Coherence Score:  0.4482491623653565\n"
     ]
    }
   ],
   "source": [
    "mar_model_pu = perform_LDA(df_march_pu['lemmatized'], mart_pu, marp_pu, mara_pu, mard_pu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "fec11ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Topic 0 ------\n",
      "@maithi_nk #ichbinreyhan problem @maithinkx thema arbeitsbedingung prekär leute system wissenschaftlich\n",
      "\n",
      "------ Topic 1 ------\n",
      "#ichbinreyhan liebe mensch @maithi_nk umfrage folge arbeitsbedingung @maithinkx @gew_bund #respectscience\n",
      "\n",
      "------ Topic 2 ------\n",
      "#ichbinreyhan kunst #wisssystemfehler arbeitsbedingung stellen prekär problem @czyina @maithi_nk hochschule\n",
      "\n",
      "------ Topic 3 ------\n",
      "streitschrift brotlose befristet jahr #ichbinreyhan @maithinkx @maithi_nk @dlf verstehen vertrag\n",
      "\n",
      "------ Topic 4 ------\n",
      "@gew_bund #dauerstell #wissenschaft @maithi_nk daueraufgabe #ichbinreyhan verstopfung stellen jahr lehre\n",
      "\n",
      "------ Topic 5 ------\n",
      "@vpod_schweiz zeit mittelbau @akellergew akademisch glückwunsch universitär @unibasel @eduint solidarity\n",
      "\n",
      "------ Topic 6 ------\n",
      "endlich jahr covid hören @drlutzboehm @maithinkx kümmern @diballestero @michael_gerloff handeln\n",
      "\n",
      "------ Topic 7 ------\n",
      "@drlutzboehm @maithinkx @diballestero #ichbinreyhan arbeitsbedingung thread argument movement know point\n",
      "\n",
      "\n",
      "Perplexity:  -7.275931958061882\n",
      "\n",
      "Coherence Score:  0.44196602227848897\n"
     ]
    }
   ],
   "source": [
    "apr_model_pu = perform_LDA(df_april_pu['lemmatized'], aprt_pu, aprp_pu, apra_pu, aprd_pu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1066dfe6",
   "metadata": {},
   "source": [
    "### Pool tweets by day and profile"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
